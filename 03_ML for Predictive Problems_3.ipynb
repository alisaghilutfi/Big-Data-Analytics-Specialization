{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import re\n",
    "import binascii\n",
    "from time import time\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "fname_ = []\n",
    "for root,_,files in os.walk(\"abstracts\"):\n",
    "    for fname in files:\n",
    "            if fname[-4:] == \".txt\":\n",
    "                fnames = os.path.join(root, fname)\n",
    "                fname_.append(fnames)\n",
    "for fn in fname_:                \n",
    "    with open(fn,  \"rt\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            for line in f:\n",
    "                if \"Abstract    :\" in line:\n",
    "                    break\n",
    "                # get abstract as a single string\n",
    "        #                             documents = [line.strip() for line in f.readlines()]\n",
    "#                 print(line)\n",
    "                documents = ' '.join([line[:-1].strip() for line in f])\n",
    "                documents = re.sub(' +', ' ', documents)  # remove double spaces\n",
    "                document.append(documents)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Type : Award NSF Org : OCE Latest Amendment Date : July 19, 1991 File : a9000333 Award Number: 9000333 Award Instr.: Standard Grant Prgm Manager: Bilal U. Haq OCE DIVISION OF OCEAN SCIENCES GEO DIRECTORATE FOR GEOSCIENCES Start Date : March 1, 1990 Expires : February 29, 1992 (Estimated) Expected Total Amt. : $37522 (Estimated) Investigator: James S. McClain Mcclain@geology.ucdavis.edu (Principal Investigator current) Sponsor : U of Cal Davis OVCR/Sponsored Programs Davis, CA 956168671 530/752-2075 NSF Program : 1620 MARINE GEOLOGY AND GEOPHYSICS Fld Applictn: 0204000 Oceanography 42 Geological Sciences Program Ref : 1577, Abstract : In the summer of 1988 the principal investigator measured microseismicity in and near the hydrothermal vent field of the Endeavor segment of the Juan de Fuca Ridge. A very surprising result was the large number (hundreds) of seismic events that were observed. The principal investigator thinks he has made the first observation of long duration tremor associated with a submarine hydrothermal field. The large number and unusual nature of these events make this data set unique. The principal investigator will: 1) construct a number-of-events vs. relative magnitude curve for as many events as possible; 2) characterize the spectra of the events; and 3) analyze the time variability of the events and compare with high-frequency data from near an active hydrothermal chimney.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for Set up (min_df = 2,use_idf = True,sublinear_tf = True,max_df =1.0,max_features = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate term-document matrix with tf-idf scores\n",
    "tfidf_vectorizer_7 = TfidfVectorizer(min_df = 2,use_idf = True,sublinear_tf = True,max_df =1.0,max_features = 20000)\n",
    "tfidf_matrix_7 = tfidf_vectorizer_7.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000', '0000099', '0000912', '001', '00709', '01', '0100', '0100000', '01002', '010033285', '0101000', '0102', '0102000', '0103000', '0104000', '0105000', '0106000', '0107000', '01075', '0108000', '0109000', '0110000', '0111', '0111000', '0112000', '0113000', '0114000', '0116000', '0118000', '0120000', '0123', '01267', '0150', '016092247', '016101400', '018543602', '02', '0201000', '0202000', '0203000', '0204000', '0205000', '0206000', '0207000', '0208000', '0211', '02114', '021155096', '021182394', '02125', '02138', '021383826', '021385317', '021394307', '02140', '02155', '024549110', '024581060', '0246', '024673800', '02481', '02543', '025431015', '02747', '02881', '028920984', '02912', '03', '0300000', '0302000', '0304000', '0304010', '0304020', '0306000', '0308000', '0309000', '0311', '0311000', '0312000', '0313000', '0313010', '0313040', '0315000', '0316000', '0317000', '0319000', '0320', '037551404', '03824', '0400000', '04011', '0411', '0430', '044695717', '04575', '05', '0510100', '0510102', '0510103', '0510203', '0510204', '0510300', '0510301', '0510402', '0510403', '0510601', '0510602', '0510604', '0510701', '0510704', '0511603', '0512004', '0516', '0521400', '0521700', '0522100', '0522400', '0527', '05405', '0601000', '062691133', '062694133', '0627', '064590241', '065208337', '0655', '0661', '0698', '07030', '071021824', '0840', '08544', '0866', '088304100', '088544103', '08901', '09', '0934', '10', '100', '1000', '10000', '100000', '1000000', '10012', '10017', '10021', '100215024', '10024', '10027', '100276598', '10031', '101', '1010', '1011', '102', '102000', '1022', '1024', '103', '1031', '1032', '1033', '1036', '1037', '1038', '1039', '104', '1041', '1043', '1044', '1045', '10458', '1049', '105', '1050', '105000', '10550', '1056', '1057', '106', '1062', '1063', '1066', '1072', '1075', '1076', '1078', '1079', '108', '1080', '108000', '1089', '109', '1091', '1092', '10920', '1094', '1096', '10be', '10cpi', '10th', '11', '110', '1100', '11000', '110000', '1101', '1104', '1106', '1107', '1108', '111', '1110', '1111', '1112', '1113', '1114', '1115', '1116', '1118', '1119', '112', '1120', '11200', '112013840', '1121', '1122', '1123', '1127', '1128', '1129', '113', '1132', '1135', '1136', '113671575', '1139', '1141', '1143', '1144', '1145', '1147', '1148', '115', '1150', '115000', '1154', '1155', '1156', '1157', '1159', '1160', '1162', '1163', '1164', '1165', '1166', '1168', '1170', '1171', '11724', '1174', '1179', '117943362', '1180', '1181', '1182', '1185', '1187', '1189', '119', '1190', '1191', '1192', '1193', '1195', '1196', '1197', '1198', '11th', '12', '120', '1200', '12000', '120000', '1201', '1205', '1206', '1207', '1210', '1214', '1215', '1216', '1217', '1218', '121803522', '1219', '1221', '12222', '1223', '1226', '1228', '1232', '1233', '1234', '1238', '124', '1241', '1242', '1244', '1245', '1248', '125', '1250', '125000', '1252', '125450129', '1260', '1261', '1262', '1263', '1264', '1265', '1266', '1267', '1268', '1269', '1271', '128', '1281', '1284', '1285', '1286', '1287', '129', '1290', '1291', '1293', '12c', '12th', '13', '130', '1300', '130000', '1303', '1304', '1305', '1306', '1308', '1309', '1311', '1312', '1313', '1314', '1315', '1317', '1318', '1319', '1320', '1321', '1322', '1323', '1324', '132441200', '1325', '1326', '1329', '1331', '1332', '1333', '13346', '1335', '1338', '1339', '1340', '1346', '135', '1350', '135000', '1352', '1353', '1357', '1358', '1360', '1366', '13699', '1371', '1372', '1385', '1387', '1389', '138th', '1390', '13901', '1391', '1392', '1393', '1397', '1398', '13c', '13th', '14', '140', '1400', '14000', '140000', '1401', '1402', '1403', '1406', '1407', '1414', '1415', '1417', '14260', '1440', '1442', '1443', '1444', '1448', '145', '145000', '14508', '1455', '1456', '146235603', '14627', '1463', '1464', '1465', '1467', '1468', '1471', '1474', '1476', '1480', '14850', '148532801', '1491', '1493', '1497', '1498', '14c', '14th', '15', '150', '1500', '15000', '150000', '1501', '1502', '1504', '1505', '1515', '1517', '1518', '1519', '152', '1520', '1521', '152133815', '1522', '1523', '1524', '1525', '1526', '152600001', '1527', '1529', '1530', '1536', '1544', '1545', '1546', '1547', '155000', '157', '1571', '1572', '1573', '1574', '1575', '1576', '1577', '1579', '1580', '1581', '1582', '1584', '1589', '1592', '1593', '1594', '15n', '15th', '16', '160', '1600', '16000', '160000', '1601', '1603', '1604', '1608', '1610', '1616', '1620', '1630', '1631', '1632', '1633', '1634', '1635', '1636', '1637', '1638', '1639', '164', '1640', '1650', '165000', '1652', '1655', '1656', '1657', '1658', '1659', '1660', '1661', '1662', '1666', '1667', '1670', '1674', '1676', '1680', '168021503', '1681', '1682', '1684', '1686', '1687', '1689', '1690', '1696', '1698', '1699', '16s', '16th', '17', '170', '1700', '17000', '170000', '1706', '1710', '1713', '1715', '1725', '1730', '173364', '1735', '1737', '1745', '175', '1750', '175000', '176043003', '1761', '1762', '1764', '1765', '1771', '1773', '1774', '1775', '17837', '1786', '1787', '1788', '17th', '18', '180', '1800', '18000', '180000', '180153005', '1803', '181', '18111', '1816', '1817', '1845', '1847', '1850', '185000', '1855', '186os', '1870', '187os', '1881', '1885', '18o', '18th', '19', '1900', '190000', '1903', '1910', '191031195', '19104', '191046205', '191226099', '1920', '1930', '1930s', '1938', '1940', '1942', '1944', '1945', '1946', '1948', '1950', '195000', '1950s', '1954', '1956', '1958', '1960', '1960s', '1962', '1964', '1965', '1966', '1968', '1969', '1970', '1970s', '1971', '19716', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '198', '1980', '1980s', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19th', '1d', '1h', '1st', '20', '200', '2000', '20000', '200000', '20001', '200051910', '200054707', '200091231', '2001', '200171556', '2002', '2003', '20036', '200372353', '2004', '2005', '20057', '200590001', '2006', '2007', '2008', '201', '2010', '2011', '202', '2020', '20233', '203', '2033', '204', '204180007', '205', '205000', '206', '20600', '2067', '207', '20706', '207425141', '2075', '208', '20814', '208143412', '208337', '20850', '208521408', '20899', '209', '2099', '20th', '21', '210', '2100', '21000', '210000', '2101', '2105', '210pb', '2111', '212', '21201', '212023101', '2121', '212182695', '2123', '21250', '213', '2131', '2136', '214', '2141', '215', '21500', '215000', '216', '2161', '216130775', '217', '218', '2186', '219', '21ne', '21st', '22', '220', '2200', '22000', '220000', '2201', '220304443', '2205', '2207', '221', '2210', '2211', '2215', '222', '2222', '22230', '223', '223143328', '224', '2241', '225', '225000', '2254', '2256', '2259', '226', '227', '228', '22807', '229', '229044195', '229069003', '23', '230', '2300', '23000', '230000', '2301', '230621346', '230th', '231', '2313', '231878795', '231pa', '232', '23298', '234', '2345', '235', '235000', '23508', '23529', '236', '23668', '237', '2385', '24', '240', '2400', '24000', '240000', '240610249', '241', '2424', '2425', '243', '244', '2444', '245', '245000', '2453', '2460', '2471', '248', '2481', '25', '250', '2500', '25000', '250000', '2503', '251', '252', '253', '2530', '2531', '2533', '254', '255', '255000', '256', '257', '259', '26', '260', '2600', '26000', '260000', '26170', '262', '263', '2631', '265000', '265066845', '266', '268', '26al', '27', '270', '2700', '27000', '270000', '27109', '273', '274', '274022465', '274110001', '275', '275000', '27514', '276', '276957514', '277', '277080077', '2777', '278', '278584353', '279', '27th', '28', '280', '2800', '28000', '280000', '2801', '2807', '282', '28223', '283', '284', '2840', '284033201', '2844', '285', '285000', '2851', '286', '2860', '2865', '287', '2876', '2880', '2884', '2885', '2886', '2888', '289', '2890', '2891', '29', '2900', '290000', '291', '292', '29208', '293', '2934', '294', '294240001', '296345702', '2977', '299', '2d', '2nd', '30', '300', '3000', '30000', '300000', '30001', '3003', '3007', '3009', '301', '302', '3021', '303', '3030', '303033044', '30322', '303320420', '3035', '304', '305', '305000', '305250', '306', '306027411', '307', '309', '3090', '31', '310', '3100', '310000', '3111', '3112', '312', '312500', '313', '3131', '3139', '314', '31411', '315', '315000', '316', '3167', '317', '318', '3181', '319', '32', '320', '3200', '32000', '320000', '3201', '321', '3211', '322', '323', '323063067', '323064166', '325', '325000', '326', '32611', '327', '328', '328263252', '329', '329016975', '33', '330', '3300', '330000', '3312', '33124', '331491098', '33199', '332', '333', '3333', '3334', '334', '334310991', '335', '3355', '336', '336209951', '337', '3370', '338', '3386', '34', '340', '3400', '340000', '341', '3411', '3417', '342', '3423', '343', '3430', '3434', '3440', '3441', '345', '345000', '345702', '346', '3466', '348', '3481', '35', '350', '3500', '35000', '350000', '3503', '352', '352940111', '353', '354870104', '355', '3550', '357', '3571', '3578', '35899', '36', '360', '3600', '36000', '360000', '3601', '361', '3616', '3622', '3634', '3640', '365', '3660', '36688', '368', '3683', '36849', '369', '36cl', '37', '370', '370000', '3705', '372', '372357749', '37240', '373', '3732', '375', '375000', '377', '378', '379', '37900', '379960140', '37th', '38', '3800', '380000', '381', '38152', '3822', '3845', '385', '38505', '386', '38677', '387', '388', '39', '3900', '390000', '3911', '392', '393', '3935', '394', '3940', '394065116', '395', '3967', '39762', '3998', '39ar', '3d', '3he', '3rd', '3res', '40', '400', '4000', '40000', '400000', '400195', '401', '4011', '402', '40292', '4031', '404', '4040', '4043', '405', '405000', '405060057', '406', '4060', '4066', '407', '408', '4080', '4083', '409', '4090', '4091', '4095', '4096', '4097', '40ar', '41', '410', '4100', '412', '413', '4134', '414', '4141', '415', '417', '4188', '419', '42', '420', '4200', '42000', '420000', '4202', '422', '423', '424', '425', '4275', '429', '4293', '43', '432', '432101016', '434', '4340', '43403', '435', '4350', '436', '436063328', '437', '438', '44', '440', '4400', '441', '441067015', '4411', '442420001', '443', '443250001', '444', '4444', '445', '447', '448', '45', '450', '4500', '45000', '450000', '45056', '4510', '452', '45202', '45221', '453', '454350001', '455', '4550', '45701', '4571', '458', '459', '46', '460', '4600', '462', '46200', '463', '465', '46556', '467', '469', '46th', '47', '470', '4700', '4705', '471', '4710', '4715', '472', '4720', '4725', '474', '474021847', '475', '4757', '4768', '477', '479', '47907', '48', '480', '4800', '48000', '480000', '4805', '481', '481091274', '482', '4820', '482023900', '483094401', '485', '486', '487', '4880', '48824', '49', '49008', '491', '492', '494', '494239000', '4949', '495', '497', '499', '499311295', '4d', '4f', '4he', '4th', '50', '500', '5000', '50000', '500000', '500112207', '501', '5014', '502', '503', '504', '505', '5050', '506', '507', '508', '509', '51', '510', '5100', '51000', '511', '5111', '5112', '5113', '5115', '5116', '512', '5121', '513', '5131', '515', '5151', '5152', '516', '517', '518', '5190', '52', '520', '5200', '5202', '5209', '5211', '5212', '5219', '5221', '52242', '5225', '523', '525', '5250', '5260', '5262', '5266', '527', '5278', '528', '5280', '529', '53', '530', '532', '5320', '532010340', '532011881', '534', '5342', '5343', '5345', '5370', '537061490', '5371', '5373', '5376', '538', '54', '540', '5400', '5405', '541', '5411', '5413', '5415', '5416', '5418', '5419', '542', '543', '545', '5465', '547', '548', '549', '549110', '55', '550', '5500', '55000', '550000', '5511', '5514', '552', '5535', '554', '554151226', '555', '556', '5567', '56', '560', '5600', '562', '5625', '563', '565', '567', '5673', '568', '57', '570', '5700', '5717', '5720', '573', '5731', '574', '5740', '575', '5750', '5761', '577', '577013995', '578', '57th', '58', '5801', '581', '581055405', '58202', '5835', '584', '585', '5878', '588', '5897', '59', '5911', '5912', '5913', '5914', '5915', '5916', '5918', '5919', '5920', '5921', '5922', '5924', '5926', '5927', '5928', '5929', '5930', '5936', '5939', '594', '5940', '5941', '5942', '5944', '5946', '5950', '5951', '5953', '5956', '5959', '596', '597', '597172470', '5974', '5976', '5977', '5978', '5979', '598', '5980', '59812', '5984', '599', '5th', '60', '600', '6000', '60000', '600000', '601', '60115', '602', '602081110', '603', '605', '606', '606052496', '606112103', '60612', '606163793', '60637', '606371404', '607', '6073', '608', '609', '61', '610', '6100', '6101', '6111', '612', '6120', '6124', '614', '615', '616', '6164', '617', '6177', '61790', '618', '61820', '619', '62', '620', '6200', '6203', '621', '6210', '622', '6221', '6222', '624', '625', '6255', '626', '6269', '627', '62901', '63', '630', '6300', '631', '631083395', '63110', '631214499', '63130', '632', '633', '6330', '635', '6387', '64', '6400', '64000', '641', '642', '6424', '643', '644', '645', '6452', '646', '648', '65', '650', '6500', '65000', '651', '652', '65211', '654091330', '656', '658', '66', '660', '66000', '660447552', '660457563', '661', '662', '663', '665061103', '666', '667', '66th', '67', '6700', '6701', '672', '673', '6741', '675', '676', '677', '6772', '6777', '678', '68', '681', '6823', '683', '684', '6840', '6845', '685', '6850', '6851', '6855', '6856', '6857', '685880430', '687', '6872', '689', '69', '690', '6900', '692', '693', '695', '696', '6th', '70', '700', '7000', '70000', '701', '7011', '701185665', '70148', '702', '703', '704', '705032701', '706', '707', '70803', '7093', '71', '7100', '7106', '7111', '713', '7137', '714', '715', '716', '7160', '717', '7172', '7173', '7174', '7179', '718', '7180', '719', '72', '720', '7200', '72000', '720130', '7202', '7211', '723', '724', '725', '7252', '7256', '7257', '7258', '7259', '7261', '7262', '7263', '727', '72701', '7293', '73', '730', '7300', '73019', '731', '7314', '732', '734', '7345', '7348', '735', '7355', '736', '7360', '7366', '737', '7381', '739', '74', '740', '7400', '74078', '7410', '741043189', '7412', '7416', '7419', '742', '7427', '7428', '7429', '743', '7430', '7432', '744', '7440', '7444', '745', '747', '7475', '748', '7482', '74999', '75', '750', '7500', '75000', '750830688', '751', '752', '75275', '753', '754', '7560', '757', '758', '76', '760190145', '761', '762', '762035250', '763', '764', '765', '766', '7666', '768', '77', '771', '772', '772042015', '772511892', '7726', '773', '774', '7749', '775', '7765', '777', '778433000', '778433578', '78', '780', '7800', '781', '78249', '783', '784', '785', '786', '78666', '787', '787137726', '79', '790', '7915', '792', '793', '794', '794091035', '795', '797', '79968', '79th', '7th', '80', '800', '8000', '80000', '8001', '801', '802', '80208', '802173364', '803', '803073000', '803090572', '804', '804011887', '805', '805232002', '806', '808', '809', '81', '8100', '812', '8121', '813', '8130', '814', '815', '816', '817', '818', '82', '820', '820713355', '820713434', '823', '824', '825', '8252', '826', '8271', '828', '8298', '83', '830688', '831', '8311', '832', '834', '835', '836', '8367', '838443010', '84', '8400', '84000', '8401', '84102', '842', '843', '843221415', '845', '846021043', '847', '85', '850', '8500', '85000', '852', '85287', '854', '855', '856', '85721', '858', '859', '86', '860', '8600', '860114130', '8602', '861', '8615', '862', '863', '864', '865', '867', '8670', '868', '8691', '86sr', '87', '870104', '871', '87131', '872', '878010389', '87sr', '88', '880', '8800', '880038001', '8804', '881', '8815', '882', '883', '885', '8877', '8888', '889', '89', '891541037', '893', '894', '895', '8953', '89557', '896', '898', '8th', '90', '900', '9000', '90000', '900241406', '9003', '900324221', '900413384', '900891147', '901', '9011', '904', '906', '907', '908', '909', '91', '910', '9102', '9103', '9104', '9106', '9107', '9108', '9109', '911250001', '9117', '9119', '912', '9120', '9123', '9125', '9126', '913', '9130', '9132', '9133', '9134', '9135', '9136', '9139', '914', '9141', '9145', '9146', '9147', '9148', '9149', '915', '9150', '9153', '9155', '9156', '9157', '916', '9161', '9162', '9163', '9164', '9165', '9169', '91711', '9177', '9178', '9179', '918', '9180', '9181', '9183', '9184', '9186', '9187', '9188', '9189', '919', '9192', '9196', '9197', '9198', '92', '9200', '920371027', '92093', '920930934', '921', '9215', '9216', '9217', '9218', '921821931', '9219', '9220', '9221', '9222', '9223', '9225', '9227', '9229', '9231', '9232', '9237', '9239', '924', '9247', '9250', '9251', '9252', '925210217', '9255', '9256', '9263', '9264', '9267', '9268', '9269', '926971875', '926977600', '9278', '928', '9282', '9283', '92831', '9285', '9290', '9292', '9294', '9295', '9296', '9297', '9299', '93', '9300', '9308', '931', '93106', '932', '934', '93407', '937', '93943', '9394512', '94', '940', '940253493', '941321722', '941430962', '94305', '94720', '949', '95', '950', '9500', '95000', '950641077', '951', '951720130', '953', '956', '956168671', '96', '960', '962', '963', '965', '966', '9660', '9661', '968222225', '969', '97', '970', '970068921', '972', '972013011', '972070751', '973', '973315503', '974', '974035219', '975', '978', '979', '98', '980093027', '980568', '981056613', '982255996', '984', '985', '989', '99', '991643140', '993', '994', '9949', '996', '997', '99775', '998', '999', '99999', '9th', 'a1', 'a2', 'aa', 'aaas', 'aaron', 'aas', 'ab', 'aba', 'abandoned', 'abandonment', 'abatement', 'abbott', 'abc', 'abdali', 'abel', 'abelian', 'aberrant', 'aberration', 'aberrations', 'abet', 'abi', 'abilene', 'abilities', 'ability', 'abiotic', 'abl', 'ablation', 'able', 'abnormal', 'abnormalities', 'aboard', 'abou', 'abound', 'about', 'above', 'abr', 'abraham', 'abrahams', 'abrasion', 'abrasive', 'abroad', 'abrupt', 'abruptly', 'abs', 'abscisic', 'abscission', 'absence', 'absent', 'absolute', 'absolutely', 'absorb', 'absorbance', 'absorbed', 'absorber', 'absorbers', 'absorbing', 'absorption', 'abstract', 'abstraction', 'abstractions', 'abstracts', 'abundance', 'abundances', 'abundant', 'abuse', 'abyssal', 'ac', 'acad', 'academe', 'academia', 'academic', 'academically', 'academics', 'academies', 'academy', 'acc', 'accel', 'accelerate', 'accelerated', 'accelerating', 'acceleration', 'accelerations', 'accelerator', 'accelerators', 'accelerometers', 'accept', 'acceptable', 'acceptance', 'accepted', 'acceptor', 'acceptors', 'access', 'accessed', 'accesses', 'accessibility', 'accessible', 'accessing', 'accessories', 'accessory', 'accident', 'accidental', 'accidents', 'acclimation', 'accommodate', 'accommodated', 'accommodates', 'accommodating', 'accommodation', 'accommodations', 'accompanied', 'accompanies', 'accompany', 'accompanying', 'accomplish', 'accomplished', 'accomplishes', 'accomplishing', 'accomplishment', 'accomplishments', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accounted', 'accounting', 'accounts', 'accreditation', 'accreted', 'accreting', 'accretion', 'accretionary', 'accrue', 'accumulate', 'accumulated', 'accumulates', 'accumulating', 'accumulation', 'accumulations', 'accuracy', 'accurate', 'accurately', 'ace', 'acetate', 'acetic', 'acetone', 'acetyl', 'acetylcholine', 'acetylene', 'achie', 'achievable', 'achieve', 'achieved', 'achievement', 'achievements', 'achieves', 'achieving', 'aci', 'acid', 'acidic', 'acidification', 'acidity', 'acids', 'ackerman', 'acknowledged', 'acm', 'acoustic', 'acoustical', 'acoustically', 'acoustics', 'acousto', 'acpub', 'acquaint', 'acquainted', 'acquire', 'acquired', 'acquires', 'acquiring', 'acquisition', 'acquisitions', 'acre', 'acres', 'across', 'acs', 'acsu', 'act', 'acted', 'actin', 'acting', 'action', 'actions', 'activ', 'activate', 'activated', 'activates', 'activating', 'activation', 'activator', 'activators', 'active', 'actively', 'activism', 'activists', 'activit', 'activiti', 'activities', 'activity', 'actor', 'actors', 'acts', 'actual', 'actually', 'actuated', 'actuation', 'actuator', 'actuators', 'acuity', 'acute', 'acyl', 'ad', 'ada', 'adam', 'adams', 'adapt', 'adaptability', 'adaptable', 'adaptation', 'adaptations', 'adapted', 'adapting', 'adaptive', 'adaptively', 'adaptivity', 'adapts', 'adc', 'adcp', 'add', 'added', 'adding', 'addison', 'addition', 'additional', 'additionally', 'additions', 'additive', 'additives', 'address', 'addressed', 'addresses', 'addressing', 'adds', 'adducts', 'adelber', 'adenosine', 'adequacy', 'adequate', 'adequately', 'adhere', 'adhesion', 'adhesive', 'adhesives', 'adiabatic', 'adic', 'adirondack', 'adjacent', 'adjoining', 'adjoint', 'adjunct', 'adjust', 'adjustable', 'adjusted', 'adjusting', 'adjustment', 'adjustments', 'adler', 'adm', 'admela', 'admin', 'administer', 'administered', 'administering', 'administration', 'administrative', 'administrator', 'administrators', 'admissible', 'admission', 'admissions', 'admit', 'admits', 'admitted', 'adolescence', 'adolescent', 'adolescents', 'adopt', 'adopted', 'adopting', 'adoption', 'adopts', 'adp', 'adrenal', 'adrian', 'ads', 'adsorb', 'adsorbate', 'adsorbates', 'adsorbed', 'adsorbent', 'adsorbents', 'adsorption', 'adult', 'adulthood', 'adults', 'advance', 'advanced', 'advancement', 'advancements', 'advances', 'advancing', 'advantage', 'advantageous', 'advantages', 'advection', 'advective', 'advent', 'adventure', 'adverse', 'adversely', 'advertised', 'advertising', 'advice', 'advise', 'advising', 'advisor', 'advisors', 'advisory', 'advocacy', 'advocated', 'advocates', 'advt', 'ae', 'aegis', 'aeolian', 'aequorin', 'aerial', 'aero', 'aerobic', 'aeroce', 'aerodynamic', 'aerodynamics', 'aerodyne', 'aerogel', 'aerogels', 'aeronautical', 'aeronautics', 'aeronomy', 'aerosol', 'aerosols', 'aerospace', 'aes', 'aesthetic', 'af', 'affairs', 'affect', 'affected', 'affecting', 'affective', 'affects', 'afferent', 'affiliate', 'affiliated', 'affiliates', 'affiliation', 'affiliations', 'affine', 'affinities', 'affinity', 'afford', 'affordable', 'afforded', 'affording', 'affords', 'afm', 'aforementioned', 'afps', 'africa', 'african', 'after', 'aftermath', 'afternoon', 'aftershocks', 'ag', 'again', 'against', 'agarwal', 'age', 'aged', 'agencies', 'agency', 'agenda', 'agendas', 'agent', 'agents', 'ages', 'agglomeration', 'aggregate', 'aggregated', 'aggregates', 'aggregation', 'aggregations', 'aggression', 'aggressive', 'aggressively', 'agile', 'aging', 'agn', 'ago', 'agr', 'agrarian', 'agrawal', 'agree', 'agreed', 'agreement', 'agreements', 'agricultural', 'agriculturally', 'agriculture', 'agrobacterium', 'agronomic', 'agu', 'ahead', 'ahluwalia', 'ahmed', 'ai', 'aid', 'aided', 'aiding', 'aids', 'aidun', 'aim', 'aimed', 'aiming', 'aims', 'ain', 'air', 'airborne', 'aircraft', 'aires', 'airflow', 'airglow', 'airline', 'airplane', 'airplanes', 'airport', 'aizenman', 'ak', 'akin', 'akkara', 'akron', 'al', 'al2o3', 'ala', 'alabama', 'alamos', 'alan', 'alanine', 'alarm', 'alaska', 'alaskan', 'albany', 'albedo', 'albeit', 'albert', 'alberta', 'alberto', 'albumin', 'albuquerque', 'albus', 'alcohol', 'alcohols', 'aldehydes', 'alder', 'alejandro', 'alert', 'aleurone', 'aleutian', 'alex', 'alexander', 'alexandra', 'alexandre', 'alexandria', 'alfalfa', 'alfons', 'alfred', 'alfven', 'alga', 'algaas', 'algae', 'algal', 'algan', 'algebra', 'algebraic', 'algebraically', 'algebras', 'algorithm', 'algorithmic', 'algorithms', 'ali', 'alice', 'alicia', 'align', 'aligned', 'aligning', 'alignment', 'alignments', 'alike', 'aliphatic', 'alison', 'alive', 'alkali', 'alkaline', 'alkalinity', 'alkaloid', 'alkaloids', 'alkane', 'alkanes', 'alkene', 'alkenes', 'alkyl', 'alkylation', 'alkynes', 'all', 'allan', 'allegheny', 'allele', 'alleles', 'allelic', 'allen', 'alleviate', 'alleviating', 'alliance', 'alliances', 'allied', 'allison', 'allocate', 'allocated', 'allocating', 'allocation', 'allocations', 'allosteric', 'allow', 'allowable', 'allowance', 'allowed', 'allowing', 'allows', 'alloy', 'alloying', 'alloys', 'alluvial', 'allyl', 'alma', 'almost', 'aln', 'aloha', 'alone', 'along', 'alongside', 'alpha', 'alpine', 'alps', 'already', 'als', 'also', 'alter', 'alteration', 'alterations', 'altered', 'altering', 'alternate', 'alternating', 'alternative', 'alternatively', 'alternatives', 'alters', 'although', 'altiplano', 'altitude', 'altitudes', 'alto', 'altogether', 'altruism', 'alumina', 'aluminide', 'aluminum', 'alumni', 'alvarez', 'alvin', 'always', 'alzheimer', 'am', 'amanda', 'amazing', 'amazon', 'amazonia', 'amazonian', 'ambient', 'ambiguities', 'ambiguity', 'ambiguous', 'ambitious', 'ameliorate', 'amenable', 'amendment', 'amer', 'america', 'american', 'americans', 'americas', 'ames', 'amherst', 'amide', 'amine', 'amines', 'amino', 'amir', 'ammonia', 'ammonium', 'amnh', 'amo', 'among', 'amongst', 'amorphous', 'amos', 'amount', 'amounts', 'amp', 'ampere', 'amphibian', 'amphibians', 'amphiphiles', 'amphiphilic', 'ample', 'amplification', 'amplified', 'amplifier', 'amplifiers', 'amplify', 'amplitude', 'amplitudes', 'ampp', 'ams', 'amsterdam', 'amt', 'amundsen', 'amy', 'an', 'ana', 'anaerobic', 'analog', 'analogies', 'analogous', 'analogs', 'analogue', 'analogues', 'analogy', 'analyses', 'analysis', 'analyst', 'analysts', 'analyte', 'analytes', 'analytic', 'analytical', 'analytically', 'analyze', 'analyzed', 'analyzer', 'analyzers', 'analyzes', 'analyzing', 'anand', 'anatomical', 'anatomically', 'anatomy', 'ancestor', 'ancestors', 'ancestral', 'ancestry', 'anchor', 'anchorage', 'anchored', 'anchoring', 'anchors', 'ancient', 'ancillary', 'and', 'andean', 'andersen', 'anderson', 'andes', 'andre', 'andrea', 'andreas', 'andrei', 'andres', 'andrew', 'andrews', 'andrill', 'androgen', 'androgens', 'andrzej', 'andy', 'anecdotal', 'anemones', 'angel', 'angela', 'angeles', 'angell', 'angelo', 'anger', 'angiosperm', 'angiosperms', 'angle', 'angles', 'angstroms', 'angular', 'anhydrase', 'ani', 'anil', 'animal', 'animals', 'animated', 'animation', 'animations', 'anion', 'anionic', 'anions', 'anir', 'anisotropic', 'anisotropies', 'anisotropy', 'anita', 'ankara', 'ann', 'anna', 'anne', 'annealing', 'annex', 'annihilation', 'annotated', 'annotation', 'annotations', 'announced', 'announcement', 'annual', 'annually', 'annular', 'anode', 'anodic', 'anomalies', 'anomalous', 'anomalously', 'anomaly', 'anonymous', 'another', 'anoxia', 'anoxic', 'answer', 'answered', 'answering', 'answers', 'ant', 'antagonists', 'antarctic', 'antarctica', 'antecedents', 'antenna', 'antennae', 'antennas', 'anterior', 'anthony', 'anthro', 'anthropogenic', 'anthropological', 'anthropologist', 'anthropologists', 'anthropology', 'anti', 'antibiotic', 'antibiotics', 'antibodies', 'antibody', 'anticipate', 'anticipated', 'anticipates', 'anticipation', 'antiferromagnetic', 'antiferromagnets', 'antifreeze', 'antigen', 'antigens', 'antilles', 'antimicrobial', 'antimony', 'antioxidant', 'antiproton', 'antiquated', 'antiquity', 'antisense', 'antiviral', 'anton', 'antonio', 'ants', 'anvil', 'anxiety', 'any', 'anyone', 'anything', 'anywhere', 'ao', 'aol', 'ap', 'ap3', 'apache', 'apart', 'apatite', 'aperiodic', 'aperture', 'apertures', 'apes', 'apex', 'aphid', 'aphids', 'apical', 'apl', 'aplysia', 'apoptosis', 'apoptotic', 'appalachian', 'appalachians', 'apparatus', 'apparel', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appeared', 'appearing', 'appears', 'appendages', 'appl', 'apple', 'appleton', 'applets', 'appliances', 'applicability', 'applicable', 'applicant', 'applicants', 'application', 'applications', 'applicats', 'applictn', 'applied', 'applies', 'apply', 'applying', 'appointed', 'appointment', 'appointments', 'appreciate', 'appreciated', 'appreciation', 'apprenticeship', 'approach', 'approached', 'approaches', 'approaching', 'appropriate', 'appropriately', 'appropriateness', 'approval', 'approved', 'approximate', 'approximated', 'approximately', 'approximates', 'approximating', 'approximation', 'approximations', 'april', 'aps', 'apt', 'aptitude', 'aquaculture', 'aquarium', 'aquatic', 'aqueous', 'aquifer', 'aquifers', 'ar', 'arab', 'arabian', 'arabic', 'arabidopsis', 'arbitrarily', 'arbitrary', 'arbor', 'arboreal', 'arc', 'arch', 'archaea', 'archaeal', 'archaebacteria', 'archaeological', 'archaeologists', 'archaeology', 'archaeometry', 'archaic', 'archbold', 'archean', 'archeological', 'archeology', 'archer', 'archipelago', 'architects', 'architectural', 'architecture', 'architectures', 'archival', 'archive', 'archived', 'archives', 'archiving', 'arcs', 'arcss', 'arctic', 'are', 'area', 'areal', 'areas', 'arecibo', 'arena', 'arenas', 'arfmp', 'argentina', 'argentine', 'arginine', 'argon', 'argonne', 'arguably', 'argue', 'argued', 'argues', 'argument', 'arguments', 'ari', 'arial', 'arid', 'arise', 'arisen', 'arises', 'arising', 'arithmetic', 'arizona', 'arkansas', 'arkes', 'arlington', 'arm', 'armed', 'arms', 'armstrong', 'army', 'arnold', 'aromatic', 'aromatics', 'arose', 'around', 'arousal', 'arpa', 'arrange', 'arranged', 'arrangement', 'arrangements', 'array', 'arrayed', 'arrays', 'arrest', 'arrested', 'arrival', 'arrivals', 'arrive', 'arrived', 'arriving', 'arsenal', 'arsenic', 'arsenide', 'art', 'arterial', 'arteries', 'artery', 'arthropod', 'arthropods', 'arthur', 'article', 'articles', 'articular', 'articulate', 'articulated', 'articulation', 'articulatory', 'artifact', 'artifacts', 'artificial', 'artificially', 'artin', 'arts', 'artsci', 'arun', 'aryl', 'arzberger', 'as', 'asa', 'asb', 'asc', 'ascent', 'ascertain', 'ascertained', 'ascomycetes', 'ascribed', 'asee', 'aseismic', 'asexual', 'ash', 'asheville', 'ashland', 'ashley', 'ashok', 'asia', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'asl', 'asp', 'aspartate', 'aspect', 'aspects', 'aspen', 'aspergillus', 'asphalt', 'aspinall', 'aspirations', 'assay', 'assayed', 'assaying', 'assays', 'assemblage', 'assemblages', 'assemble', 'assembled', 'assembles', 'assemblies', 'assembling', 'assembly', 'assertion', 'assertions', 'asserts', 'assess', 'assessed', 'assesses', 'assessing', 'assessment', 'assessments', 'asset', 'assets', 'assign', 'assigned', 'assigning', 'assignment', 'assignments', 'assimilate', 'assimilated', 'assimilation', 'assist', 'assistance', 'assistant', 'assistants', 'assistantships', 'assisted', 'assisting', 'assistive', 'assists', 'assn', 'assoc', 'associate', 'associated', 'associates', 'associating', 'association', 'associations', 'associative', 'assume', 'assumed', 'assumes', 'assuming', 'assumption', 'assumptions', 'assurance', 'assure', 'assured', 'assuring', 'ast', 'asteraceae', 'asterisks', 'asteroid', 'asteroids', 'asthenosphere', 'astill', 'astonishing', 'astro', 'astron', 'astronomers', 'astronomical', 'astronomy', 'astrophys', 'astrophysc', 'astrophysical', 'astrophysics', 'asu', 'asymmetric', 'asymmetries', 'asymmetry', 'asymptotic', 'asymptotically', 'asymptotics', 'asynchronous', 'at', 'ate', 'athens', 'atiyah', 'atkinson', 'atlanta', 'atlantic', 'atlantis', 'atlas', 'atm', 'atmos', 'atmosphere', 'atmospheres', 'atmospheric', 'atom', 'atomic', 'atomically', 'atomistic', 'atomization', 'atoms', 'atp', 'atpase', 'atpases', 'att', 'attach', 'attached', 'attaching', 'attachment', 'attachments', 'attack', 'attacked', 'attacking', 'attacks', 'attain', 'attainable', 'attained', 'attaining', 'attainment', 'attempt', 'attempted', 'attempting', 'attempts', 'attend', 'attendance', 'attendant', 'attended', 'attendees', 'attending', 'attention', 'attentional', 'attenuated', 'attenuation', 'attitude', 'attitudes', 'attitudinal', 'attorneys', 'attract', 'attracted', 'attracting', 'attraction', 'attractive', 'attractiveness', 'attractor', 'attractors', 'attracts', 'attributable', 'attribute', 'attributed', 'attributes', 'attrition', 'au', 'aubrey', 'auburn', 'auction', 'auctions', 'audience', 'audiences', 'audio', 'audit', 'auditory', 'auger', 'augment', 'augmentatio', 'augmentation', 'augmented', 'augmenting', 'augments', 'augsburg', 'august', 'augustana', 'aurora', 'auroral', 'auspices', 'austin', 'austral', 'australia', 'australian', 'austria', 'authentic', 'authentication', 'authigenic', 'author', 'authored', 'authoring', 'authorities', 'authority', 'authors', 'auto', 'autocorrelation', 'automata', 'automate', 'automated', 'automatic', 'automatically', 'automating', 'automation', 'automaton', 'automobile', 'automobiles', 'automorphic', 'automorphism', 'automorphisms', 'automotive', 'autonoma', 'autonomic', 'autonomous', 'autonomously', 'autonomy', 'autoregressive', 'autotrophic', 'auv', 'aux', 'auxiliary', 'auxin', 'avail', 'availability', 'available', 'avalanche', 'avalanches', 'ave', 'avenue', 'avenues', 'average', 'averaged', 'averages', 'averaging', 'aversion', 'avian', 'aviation', 'avijit', 'avoid', 'avoidance', 'avoided', 'avoiding', 'avoids', 'award', 'awarded', 'awardee', 'awardees', 'awards', 'awards96', 'aware', 'awareness', 'away', 'awds', 'axes', 'axial', 'axiom', 'axiomatic', 'axioms', 'axis', 'axisymmetric', 'axon', 'axonal', 'axons', 'ayres', 'az', 'b5', 'ba', 'babbitt', 'baboons', 'bac', 'baccalaureate', 'bachelor', 'bacillus', 'back', 'backbone', 'background', 'backgrounds', 'backlog', 'backscatter', 'backscattered', 'backscattering', 'backup', 'backward', 'bacteria', 'bacterial', 'bacteriophage', 'bacterioplankton', 'bacteriorhodopsin', 'bacterium', 'baculovirus', 'bad', 'badly', 'baer', 'baerwald', 'baffin', 'bag', 'bahamas', 'baheti', 'baier', 'baikal', 'bailey', 'bainbridge', 'baja', 'baker', 'balance', 'balanced', 'balances', 'balancing', 'baldwin', 'ball', 'ballistic', 'balloon', 'balloons', 'ballou', 'balls', 'baltic', 'baltimore', 'banach', 'bancroft', 'band', 'bandgap', 'banding', 'bands', 'bandwidth', 'bandwidths', 'banerjee', 'bang', 'bangalore', 'bangladesh', 'bank', 'banking', 'bankruptcy', 'banks', 'bar', 'barak', 'barbados', 'barbara', 'barber', 'bard', 'bardet', 'bare', 'barents', 'bargaining', 'barium', 'bark', 'barker', 'barley', 'barnard', 'barnes', 'barnett', 'baroclinic', 'barotropic', 'barratt', 'barrel', 'barrera', 'barrett', 'barrier', 'barriers', 'barron', 'barrow', 'barry', 'bars', 'bart', 'barth', 'bartlett', 'bartol', 'bartoli', 'barton', 'baruch', 'baryon', 'baryons', 'basal', 'basalt', 'basaltic', 'basalts', 'base', 'based', 'baseline', 'basement', 'bases', 'basic', 'basically', 'basics', 'basin', 'basins', 'basis', 'bass', 'basse', 'basu', 'bat', 'batch', 'bates', 'bath', 'batholith', 'bathymetric', 'bathymetry', 'batiza', 'baton', 'bats', 'batteries', 'battery', 'bauer', 'baum', 'baxter', 'bay', 'bayer', 'bayes', 'bayesian', 'baylor', 'bays', 'bbsr', 'bc', 'bcc', 'bcm', 'bcs', 'bd', 'be', 'beach', 'beaches', 'beacon', 'bead', 'beads', 'beam', 'beamline', 'beams', 'bean', 'bear', 'bearing', 'bearings', 'bears', 'beating', 'beaufort', 'beautiful', 'beauty', 'beaver', 'beaverton', 'bec', 'became', 'because', 'beck', 'becker', 'beckman', 'become', 'becomes', 'becoming', 'bed', 'bedford', 'bedload', 'bedrock', 'beds', 'bee', 'been', 'bees', 'beetle', 'beetles', 'before', 'began', 'begin', 'beginning', 'beginnings', 'begins', 'begun', 'behalf', 'behav', 'behave', 'behaved', 'behaves', 'behaving', 'behavior', 'behavioral', 'behaviorally', 'behaviors', 'behaviour', 'behind', 'beijing', 'being', 'beings', 'belgium', 'belief', 'beliefs', 'believe', 'believed', 'believes', 'belize', 'bell', 'bellevue', 'bellingham', 'beloit', 'belong', 'belonging', 'belongs', 'below', 'belowground', 'belt', 'belts', 'ben', 'bench', 'benches', 'benchmark', 'benchmarking', 'benchmarks', 'bend', 'bender', 'bending', 'bends', 'beneath', 'beneficial', 'beneficiaries', 'benefit', 'benefited', 'benefiting', 'benefits', 'benign', 'benjamin', 'bennett', 'benson', 'bent', 'benthic', 'benthos', 'bentley', 'benzene', 'beowulf', 'berg', 'bergen', 'berger', 'bergman', 'bering', 'beringia', 'berkeley', 'berkowitz', 'berlin', 'berman', 'bermuda', 'bernard', 'bernardino', 'bernasek', 'bernd', 'bernhard', 'bernice', 'bernoulli', 'bernstein', 'berry', 'bert', 'beryllium', 'bes', 'besides', 'best', 'beta', 'beth', 'bethel', 'bethesda', 'bethlehem', 'better', 'betty', 'between', 'beverly', 'beyond', 'bgc', 'bgsu', 'bhavani', 'bi', 'bias', 'biased', 'biases', 'biaxial', 'bibliographic', 'bibliography', 'bid', 'bidders', 'bidding', 'bids', 'biennial', 'bifunctional', 'bifurcation', 'bifurcations', 'big', 'bigelow', 'bight', 'bilal', 'bilateral', 'bilayer', 'bilayers', 'bilham', 'bilinear', 'bilingual', 'bill', 'billiards', 'billion', 'billions', 'bills', 'bima', 'bimetallic', 'bimolecular', 'bin', 'binaries', 'binary', 'binaut', 'bind', 'binder', 'binding', 'binds', 'binghamton', 'binocular', 'binuclear', 'bio', 'bioactive', 'bioanalytical', 'bioassay', 'bioassays', 'bioavailability', 'biocatalysis', 'biochem', 'biochemical', 'biochemically', 'biochemistry', 'biochemists', 'biocompatibility', 'biocompatible', 'biocomplexity', 'biodegradable', 'biodegradation', 'biodiversity', 'bioenergetics', 'bioengineering', 'bioethics', 'biofilm', 'biofilms', 'biogenesis', 'biogenic', 'biogeochemical', 'biogeochemistry', 'biogeographic', 'biogeographical', 'biogeography', 'bioinformatics', 'bioinorganic', 'biol', 'biolo', 'biologic', 'biological', 'biologically', 'biologist', 'biologists', 'biology', 'bioluminescence', 'biomarker', 'biomarkers', 'biomass', 'biomaterial', 'biomaterials', 'biome', 'biomechanical', 'biomechanics', 'biomedical', 'biomedicine', 'biomes', 'biomimetic', 'biomineralization', 'biomolec', 'biomolecular', 'biomolecules', 'bioorganic', 'biophysical', 'biophysics', 'biopolymer', 'biopolymers', 'bioprocessing', 'bioreactor', 'bioreactors', 'bioremediation', 'biosci', 'bioscience', 'biosciences', 'biosensor', 'biosensors', 'biosphere', 'biostratigraphic', 'biostratigraphy', 'biosynthesis', 'biosynthetic', 'biosystems', 'biot', 'biota', 'biotech', 'biotechnological', 'biotechnology', 'biotic', 'biotin', 'bipolar', 'bir', 'biradicals', 'birational', 'birch', 'bird', 'birds', 'birefringence', 'birmingham', 'birnbaum', 'birth', 'bis', 'biscayne', 'bishop', 'bismuth', 'bison', 'bit', 'bitnet', 'bits', 'bivalve', 'bivalves', 'black', 'blacks', 'blacksburg', 'blackwell', 'blade', 'blades', 'blair', 'blake', 'blanco', 'blank', 'blansitt', 'blast', 'bldg', 'bleaching', 'blend', 'blended', 'blending', 'blends', 'blind', 'bloch', 'block', 'blockade', 'blocked', 'blocking', 'blocks', 'blodgett', 'blood', 'bloom', 'bloomfield', 'bloomington', 'blooms', 'blossomed', 'blot', 'blots', 'blotting', 'blount', 'blow', 'blue', 'blueprint', 'bluff', 'blum', 'blumenthal', 'blvd', 'bme', 'bmp', 'bn', 'bo', 'boa', 'board', 'boards', 'boat', 'bob', 'boca', 'bock', 'bodied', 'bodies', 'body', 'boeing', 'bogdan', 'boiling', 'boise', 'bolivia', 'bolivian', 'bollam', 'bolt', 'boltzmann', 'bomb', 'bombardment', 'bombay', 'bond', 'bonded', 'bonding', 'bonds', 'bone', 'bones', 'bonnee', 'bonney', 'bonnie', 'bony', 'book', 'booker', 'books', 'boolean', 'boom', 'boone', 'boost', 'boothbay', 'bootstrap', 'borane', 'borate', 'border', 'borderland', 'borders', 'bore', 'boreal', 'borehole', 'boreholes', 'borel', 'borg', 'boring', 'boris', 'born', 'borne', 'boron', 'borrowed', 'borrowing', 'bose', 'boson', 'boston', 'botanical', 'botanists', 'botany', 'both', 'bottleneck', 'bottlenecks', 'bottles', 'bottom', 'boulder', 'boulevard', 'bound', 'boundaries', 'boundary', 'bounded', 'boundedness', 'bounding', 'bounds', 'bourgeois', 'bovine', 'bow', 'bowdoin', 'bowen', 'bower', 'bowers', 'bowling', 'bowman', 'bowne', 'box', 'boxes', 'boyce', 'boyd', 'boyer', 'boylan', 'boyle', 'boys', 'bozeman', 'bp', 'br', 'brachiopods', 'brad', 'bradford', 'bradley', 'brady', 'bragg', 'bragin', 'braid', 'braided', 'braille', 'brain', 'brains', 'brainstem', 'brake', 'branch', 'branched', 'branches', 'branching', 'brand', 'brandeis', 'brandon', 'brandt', 'bransfield', 'brassica', 'braun', 'brawley', 'bray', 'brazil', 'brazilian', 'breadth', 'break', 'breakage', 'breakdown', 'breaking', 'breakout', 'breaks', 'breakthrough', 'breakthroughs', 'breakup', 'breast', 'breath', 'breathing', 'breckler', 'breed', 'breeders', 'breeding', 'brempong', 'brenda', 'brenner', 'brent', 'brett', 'brewer', 'brian', 'briar', 'bridge', 'bridged', 'bridges', 'bridgewater', 'bridging', 'brief', 'briefly', 'brien', 'briggs', 'brigham', 'bright', 'brightness', 'brillouin', 'brine', 'brines', 'bring', 'bringing', 'brings', 'bristol', 'britain', 'british', 'britt', 'brittle', 'broad', 'broadband', 'broadcast', 'broadcasting', 'broadcasts', 'broaden', 'broadened', 'broadening', 'broadens', 'broader', 'broadest', 'broadly', 'broadway', 'brock', 'brockport', 'broken', 'bromide', 'bromine', 'bronx', 'bronze', 'brood', 'brook', 'brookhaven', 'brookings', 'brooklyn', 'brooks', 'brought', 'brown', 'brownian', 'browning', 'browser', 'browsers', 'browsing', 'bruce', 'bruno', 'brunswick', 'brush', 'bryan', 'bryant', 'bryn', 'bs', 'bt', 'bu', 'bubble', 'bubbles', 'buchtel', 'buck', 'buckley', 'buckling', 'bucknell', 'bud', 'budapest', 'budding', 'budget', 'budgets', 'buenos', 'buffalo', 'buffer', 'buffered', 'buffering', 'buffers', 'bugs', 'build', 'builders', 'building', 'buildings', 'builds', 'buildup', 'built', 'bulgaria', 'bulge', 'bulk', 'bulky', 'bull', 'bulletin', 'bundle', 'bundles', 'buoy', 'buoyancy', 'buoyant', 'buoys', 'burden', 'bureau', 'bureaucratic', 'burgeoning', 'burger', 'burial', 'burials', 'buried', 'burka', 'burke', 'burlington', 'burn', 'burned', 'burner', 'burnett', 'burning', 'burns', 'burrows', 'burruss', 'burst', 'bursting', 'bursts', 'bursty', 'burton', 'bus', 'busch', 'bush', 'business', 'businesses', 'buss', 'but', 'butcher', 'butler', 'butte', 'butterflies', 'butterfly', 'buy', 'buyer', 'buyers', 'by', 'byers', 'bynum', 'bypass', 'byproduct', 'byproducts', 'byrd', 'byrne', 'byrnes', 'byu', 'c02', 'c1', 'c2', 'c3', 'c4', 'c60', 'ca', 'ca2', 'cabinets', 'cable', 'cables', 'cache', 'caches', 'caching', 'caco3', 'cad', 'cadmium', 'cadre', 'cae', 'caenorhabditis', 'cage', 'cages', 'cai', 'cairo', 'cal', 'calabi', 'calanus', 'calcareous', 'calcification', 'calcite', 'calcium', 'calculate', 'calculated', 'calculating', 'calculation', 'calculational', 'calculations', 'calculator', 'calculators', 'calculus', 'caldera', 'caldwell', 'calendar', 'calhoun', 'calibrate', 'calibrated', 'calibrating', 'calibration', 'calibrations', 'california', 'call', 'called', 'calling', 'calls', 'calmodulin', 'calorimeter', 'calorimetric', 'calorimetry', 'caltech', 'calvin', 'calving', 'cam', 'cambrian', 'cambridge', 'camden', 'came', 'camera', 'cameras', 'cameron', 'camino', 'camp', 'campaign', 'campaigns', 'campanile', 'campbell', 'camps', 'campus', 'campuses', 'can', 'canada', 'canadian', 'canal', 'canals', 'canary', 'canberra', 'cancellation', 'cancer', 'candidate', 'candidates', 'cane', 'cannon', 'cannot', 'canonical', 'canopies', 'canopy', 'cantilever', 'cantilevers', 'cantor', 'canyon', 'canyons', 'cao', 'cap', 'capabilities', 'capability', 'capable', 'capacitance', 'capacities', 'capacitive', 'capacitor', 'capacitors', 'capacity', 'cape', 'capen', 'capillary', 'capital', 'capitalism', 'capitalist', 'capitalize', 'capitalizes', 'caplan', 'capped', 'caps', 'capsid', 'capstone', 'capsule', 'capsules', 'captive', 'capture', 'captured', 'captures', 'capturing', 'car', 'carbene', 'carbenes', 'carbide', 'carbides', 'carbohydrate', 'carbohydrates', 'carbon', 'carbonaceous', 'carbonate', 'carbonates', 'carbondale', 'carbonic', 'carboniferous', 'carbons', 'carbonyl', 'carboxylase', 'carboxylic', 'card', 'cardiac', 'cardinal', 'cardinals', 'cardiovascular', 'cards', 'care', 'career', 'careers', 'careful', 'carefully', 'carey', 'cargo', 'cariaco', 'caribbean', 'carl', 'carla', 'carleton', 'carlisle', 'carlo', 'carlos', 'carlson', 'carlyle', 'carmen', 'carnegie', 'carnivores', 'carol', 'carole', 'carolina', 'caroline', 'carolyn', 'carotenoid', 'carotenoids', 'carpenter', 'carr', 'carried', 'carrier', 'carriers', 'carries', 'carroll', 'carry', 'carrying', 'cars', 'carson', 'carter', 'cartesian', 'cartilage', 'cartography', 'cary', 'cas', 'casa', 'cascade', 'cascades', 'cascadia', 'cascading', 'case', 'cases', 'casey', 'cash', 'cashman', 'cass', 'cassandra', 'cassel', 'cassette', 'cassidy', 'casson', 'cast', 'caste', 'castillo', 'casting', 'castle', 'castro', 'casual', 'cat', 'catabolic', 'catabolism', 'cataclysmic', 'catalog', 'cataloged', 'cataloging', 'catalogs', 'catalogue', 'catalogued', 'catalysis', 'catalyst', 'catalysts', 'catalytic', 'catalytically', 'catalyze', 'catalyzed', 'catalyzes', 'catalyzing', 'catastrophic', 'catch', 'catchment', 'catchments', 'categorical', 'categories', 'categorization', 'categorize', 'categorized', 'category', 'catenin', 'caterpillar', 'caterpillars', 'catfish', 'catherine', 'cathode', 'cathodes', 'cathodoluminescence', 'catholic', 'cathy', 'cation', 'cationic', 'cations', 'cats', 'cattle', 'cau', 'cauchy', 'caught', 'causal', 'causality', 'causation', 'causative', 'cause', 'caused', 'causes', 'causeway', 'causing', 'cavanaugh', 'cave', 'caves', 'cavitation', 'cavities', 'cavity', 'cbl', 'cbms', 'cbs', 'cc', 'ccd', 'cchem', 'ccli', 'ccm', 'ccmail', 'ccmr', 'ccn', 'ccny', 'ccr', 'ccs', 'ccvd', 'cd', 'cda', 'cdf', 'cdma', 'cdna', 'cdnas', 'cdw', 'ce', 'cebaf', 'cecile', 'cecily', 'cedar', 'celebrated', 'celeste', 'celestial', 'cell', 'celled', 'cells', 'cellular', 'cellulose', 'cem', 'cement', 'cements', 'cen', 'cenozoic', 'censored', 'censoring', 'census', 'censuses', 'cent', 'centennial', 'center', 'centered', 'centering', 'centerpiece', 'centers', 'centimeter', 'centimeters', 'central', 'centralization', 'centralized', 'centrally', 'centre', 'centric', 'centrifugal', 'centrifugation', 'centrifuge', 'centrifuges', 'centro', 'centuries', 'century', 'ceramic', 'ceramics', 'cereal', 'cerebellar', 'cerebellum', 'cerebral', 'ceremonial', 'cerevisiae', 'cern', 'cerro', 'certain', 'certainly', 'certainty', 'certificate', 'certification', 'certified', 'ces', 'cesium', 'cessation', 'cetaceans', 'cetp', 'cf', 'cfa', 'cfc', 'cfcs', 'cfd', 'cfm', 'cft', 'cgi', 'cgmp', 'cgp', 'ch', 'ch4', 'chad', 'chain', 'chains', 'chair', 'chaired', 'chairman', 'chairs', 'chalcogenide', 'chalcogenides', 'challenge', 'challenged', 'challenges', 'challenging', 'chalmers', 'chamber', 'chamberlain', 'chambers', 'champaign', 'chan', 'chance', 'chances', 'chandler', 'chandra', 'chang', 'change', 'changed', 'changes', 'changing', 'channel', 'channeling', 'channels', 'chao', 'chaos', 'chaotic', 'chaparral', 'chapel', 'chaperone', 'chaperones', 'chapin', 'chapman', 'chapter', 'chapters', 'character', 'characteristic', 'characteristics', 'characterization', 'characterizations', 'characterize', 'characterized', 'characterizes', 'characterizing', 'characters', 'charalabos', 'charcoal', 'charge', 'charged', 'charges', 'charging', 'charitable', 'charles', 'charleston', 'charlotte', 'charlottesville', 'charm', 'chart', 'charter', 'charts', 'chase', 'chattanooga', 'chatter', 'chaudhuri', 'che', 'cheap', 'cheaper', 'check', 'checked', 'checking', 'checks', 'cheena', 'chem', 'cheme', 'chemical', 'chemically', 'chemicals', 'chemisorption', 'chemist', 'chemistries', 'chemistry', 'chemists', 'chemosensory', 'chemotaxis', 'chen', 'cheney', 'cheng', 'chern', 'cherniavsky', 'cheryl', 'chesapeake', 'chess', 'chester', 'chestnut', 'chewing', 'chi', 'chiang', 'chicago', 'chick', 'chicken', 'chicks', 'chico', 'chief', 'chiefly', 'chieh', 'chien', 'chih', 'child', 'childhood', 'children', 'chile', 'chilean', 'chill', 'chimeras', 'chimeric', 'chimpanzee', 'chimpanzees', 'chin', 'china', 'chinese', 'ching', 'chip', 'chips', 'chiral', 'chirality', 'chitin', 'chitnis', 'chiu', 'chlamydomonas', 'chloride', 'chlorinated', 'chlorine', 'chlorofluorocarbons', 'chlorophyll', 'chloroplast', 'chloroplasts', 'cho', 'choi', 'choice', 'choices', 'cholesterol', 'cholinergic', 'chong', 'choose', 'choosing', 'chosen', 'chow', 'chris', 'christensen', 'christian', 'christie', 'christina', 'christine', 'christopher', 'christos', 'chromatic', 'chromatin', 'chromatograph', 'chromatographic', 'chromatography', 'chromium', 'chromodynamics', 'chromophore', 'chromophores', 'chromosomal', 'chromosome', 'chromosomes', 'chronic', 'chronological', 'chronologies', 'chronology', 'chronostratigraphic', 'chronostratigraphy', 'chu', 'chukchi', 'chun', 'chung', 'church', 'ci', 'cilia', 'ciliate', 'ciliated', 'ciliates', 'cim', 'cims', 'cincinnati', 'cindy', 'circadian', 'circle', 'circles', 'circuit', 'circuitry', 'circuits', 'circular', 'circularly', 'circulated', 'circulating', 'circulation', 'circulations', 'circulatory', 'circum', 'circumpolar', 'circumstances', 'circumstellar', 'circumvent', 'cirrus', 'cis', 'cise', 'citation', 'citations', 'cited', 'cities', 'citizen', 'citizens', 'citizenship', 'citrus', 'citters', 'city', 'civic', 'civil', 'civilian', 'civilization', 'civilizations', 'ciw', 'cl', 'cladding', 'clade', 'clades', 'cladistic', 'claim', 'claimed', 'claims', 'claire', 'clamp', 'clamping', 'clams', 'clancey', 'clara', 'claremont', 'clarence', 'clarification', 'clarified', 'clarify', 'clarifying', 'clarity', 'clark', 'clarke', 'clarkson', 'clarku', 'clas', 'class', 'classes', 'classic', 'classical', 'classically', 'classification', 'classifications', 'classified', 'classifier', 'classify', 'classifying', 'classroom', 'classrooms', 'clast', 'clastic', 'clasts', 'clathrin', 'claude', 'claudia', 'clause', 'clay', 'clays', 'clayton', 'clean', 'cleaner', 'cleaning', 'cleanup', 'clear', 'clearance', 'clearer', 'clearing', 'clearinghouse', 'clearly', 'cleavage', 'cleave', 'cleaved', 'cleft', 'clemens', 'clement', 'clemson', 'clesceri', 'cleveland', 'client', 'clients', 'cliff', 'clifford', 'clifton', 'climate', 'climates', 'climatic', 'climatically', 'climatological', 'climatology', 'cline', 'clinic', 'clinical', 'clinically', 'clinicians', 'clinopyroxene', 'clinton', 'clips', 'clock', 'clocks', 'clonal', 'clone', 'cloned', 'clones', 'cloning', 'close', 'closed', 'closely', 'closer', 'closest', 'closing', 'closure', 'closures', 'clothing', 'cloud', 'clouds', 'clover', 'clu', 'club', 'clubs', 'clues', 'cluster', 'clustered', 'clustering', 'clusters', 'clutch', 'clyde', 'cm', 'cm2', 'cmb', 'cmc', 'cme', 'cmm', 'cmos', 'cmp', 'cmr', 'cms', 'cmty', 'cmu', 'cn', 'cnc', 'cnrs', 'cns', 'co', 'co2', 'coa', 'coaching', 'coagulation', 'coal', 'coalescence', 'coalition', 'coalitions', 'coare', 'coarse', 'coarsening', 'coas', 'coast', 'coastal', 'coastline', 'coasts', 'coat', 'coated', 'coating', 'coatings', 'cobalt', 'cobb', 'cobordism', 'coburn', 'cochlear', 'cochran', 'cocking', 'cod', 'code', 'coded', 'coders', 'codes', 'codimension', 'coding', 'codon', 'coe', 'coefficient', 'coefficients', 'coenzyme', 'coeval', 'coevolution', 'coevolutionary', 'coexist', 'coexistence', 'coexisting', 'cofactor', 'cofactors', 'coffee', 'cognate', 'cognition', 'cognitive', 'cohen', 'coherence', 'coherent', 'coherently', 'cohesion', 'cohesive', 'cohomological', 'cohomology', 'cohort', 'cohorts', 'coil', 'coiled', 'coils', 'coincide', 'coincidence', 'coincident', 'coincides', 'col', 'colby', 'cold', 'cole', 'coleman', 'colgate', 'coli', 'colin', 'coll', 'collaborate', 'collaborated', 'collaborating', 'collaboration', 'collaborations', 'collaborative', 'collaboratively', 'collaborator', 'collaborators', 'collaboratory', 'collagen', 'collapse', 'collapsed', 'collapsing', 'colleague', 'colleagues', 'collect', 'collected', 'collecting', 'collection', 'collections', 'collective', 'collectively', 'collector', 'collectors', 'collects', 'college', 'colleges', 'collegial', 'collegiate', 'collide', 'collider', 'colliders', 'colliding', 'collier', 'collimated', 'collins', 'collision', 'collisional', 'collisionless', 'collisions', 'collocation', 'colloid', 'colloidal', 'colloids', 'colloquia', 'colloquium', 'colman', 'colombia', 'colon', 'colonel', 'colonial', 'colonies', 'colonization', 'colonize', 'colonized', 'colony', 'color', 'colorado', 'coloration', 'colored', 'coloring', 'colors', 'colossal', 'colostate', 'columbia', 'columbus', 'column', 'columnar']\n",
      "['zoo', 'zoological', 'zoologists', 'zoology', 'zoom', 'zooplankton', 'zr', 'zucker', 'zurich', 'zygotic']\n"
     ]
    }
   ],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer_7.get_feature_names()[:5120])\n",
    "print(tfidf_vectorizer_7.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer_7.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132041\tabstract\n",
      "132041\tapplictn\n",
      "132041\tfld\n",
      "132041\tprogram\n",
      "132041\tsponsor\n",
      "132041\tinvestigator\n",
      "132041\tamt\n",
      "132041\ttotal\n",
      "132041\texpected\n",
      "132041\texpires\n",
      "132041\tstart\n",
      "132041\tmanager\n",
      "132041\tprgm\n",
      "132041\tinstr\n",
      "132041\tnumber\n",
      "132041\tfile\n",
      "132041\tdate\n",
      "132041\tamendment\n",
      "132041\tlatest\n",
      "132041\torg\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer_7.inverse_transform(tfidf_matrix_7)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For set up (min_df = .3,use_idf = True,sublinear_tf = True,max_df =1.0,max_features = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate term-document matrix with tf-idf scores\n",
    "tfidf_vectorizer_6 = TfidfVectorizer(min_df = .3,use_idf = True,sublinear_tf = True,max_df =1.0,max_features = 20000)\n",
    "tfidf_matrix_6 = tfidf_vectorizer_6.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000', '0000099', '15', '30', '31', 'abstract', 'amendment', 'amt', 'an', 'and', 'applications', 'applictn', 'are', 'as', 'at', 'august', 'award', 'be', 'by', 'co', 'continuing', 'current', 'date', 'direct', 'div', 'division', 'edu', 'estimated', 'expected', 'expires', 'file', 'fld', 'for', 'from', 'grant', 'has', 'have', 'in', 'instr', 'investigator', 'is', 'it', 'july', 'latest', 'manager', 'may', 'nec', 'new', 'nsf', 'number', 'of', 'on', 'or', 'org', 'other', 'othr', 'prgm', 'principal', 'program', 'project', 'ref', 'research', 'science', 'sciences', 'sponsor', 'standard', 'start', 'study', 'that', 'the', 'their', 'these', 'this', 'to', 'total', 'type', 'university', 'which', 'will', 'with']\n",
      "['their', 'these', 'this', 'to', 'total', 'type', 'university', 'which', 'will', 'with']\n"
     ]
    }
   ],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer_6.get_feature_names()[:5120])\n",
    "print(tfidf_vectorizer_6.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer_6.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132041\tabstract\n",
      "132041\tapplictn\n",
      "132041\tfld\n",
      "132041\tprogram\n",
      "132041\tsponsor\n",
      "132041\tinvestigator\n",
      "132041\tamt\n",
      "132041\ttotal\n",
      "132041\texpected\n",
      "132041\texpires\n",
      "132041\tstart\n",
      "132041\tmanager\n",
      "132041\tprgm\n",
      "132041\tinstr\n",
      "132041\tnumber\n",
      "132041\tfile\n",
      "132041\tdate\n",
      "132041\tamendment\n",
      "132041\tlatest\n",
      "132041\torg\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer_6.inverse_transform(tfidf_matrix_6)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For set up (min_df = .3,use_idf = True,sublinear_tf = True,max_df =0.8,max_features = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate term-document matrix with tf-idf scores\n",
    "tfidf_vectorizer_5 = TfidfVectorizer(min_df = .3,use_idf = True,sublinear_tf = True,max_df =0.8,max_features = 20000)\n",
    "tfidf_matrix_5= tfidf_vectorizer_5.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000', '0000099', '15', '30', '31', 'an', 'applications', 'are', 'as', 'at', 'august', 'be', 'by', 'co', 'continuing', 'direct', 'div', 'division', 'edu', 'from', 'has', 'have', 'is', 'it', 'july', 'may', 'nec', 'new', 'on', 'or', 'other', 'othr', 'project', 'research', 'science', 'sciences', 'standard', 'study', 'that', 'their', 'these', 'university', 'which', 'will', 'with']\n",
      "['sciences', 'standard', 'study', 'that', 'their', 'these', 'university', 'which', 'will', 'with']\n"
     ]
    }
   ],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer_5.get_feature_names()[:5120])\n",
    "print(tfidf_vectorizer_5.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer_5.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104541\twill\n",
      "103120\tis\n",
      "93797\tbe\n",
      "92379\tdirect\n",
      "88634\ton\n",
      "86147\tresearch\n",
      "85032\twith\n",
      "84619\tedu\n",
      "83008\t31\n",
      "80661\tother\n",
      "79187\tstandard\n",
      "78064\tare\n",
      "76765\tby\n",
      "76560\tthat\n",
      "72314\tas\n",
      "71128\tapplications\n",
      "69766\tdivision\n",
      "67087\tuniversity\n",
      "66869\tsciences\n",
      "66028\tfrom\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer_5.inverse_transform(tfidf_matrix_5)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For set up (min_df = .3,use_idf = True,sublinear_tf = True,max_df =0.7,max_features = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate term-document matrix with tf-idf scores\n",
    "tfidf_vectorizer_4 = TfidfVectorizer(min_df = .3,use_idf = True,sublinear_tf = True,max_df =0.7,max_features = 20000)\n",
    "tfidf_matrix_4 = tfidf_vectorizer_4.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000', '0000099', '15', '30', '31', 'an', 'applications', 'are', 'as', 'at', 'august', 'by', 'co', 'continuing', 'direct', 'div', 'division', 'edu', 'from', 'has', 'have', 'it', 'july', 'may', 'nec', 'new', 'on', 'or', 'other', 'othr', 'project', 'research', 'science', 'sciences', 'standard', 'study', 'that', 'their', 'these', 'university', 'which', 'with']\n",
      "['science', 'sciences', 'standard', 'study', 'that', 'their', 'these', 'university', 'which', 'with']\n"
     ]
    }
   ],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer_4.get_feature_names()[:5120])\n",
    "print(tfidf_vectorizer_4.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer_4.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92379\tdirect\n",
      "88634\ton\n",
      "86147\tresearch\n",
      "85032\twith\n",
      "84619\tedu\n",
      "83008\t31\n",
      "80661\tother\n",
      "79187\tstandard\n",
      "78064\tare\n",
      "76765\tby\n",
      "76560\tthat\n",
      "72314\tas\n",
      "71128\tapplications\n",
      "69766\tdivision\n",
      "67087\tuniversity\n",
      "66869\tsciences\n",
      "66028\tfrom\n",
      "65961\tan\n",
      "65876\tnec\n",
      "60811\tat\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer_4.inverse_transform(tfidf_matrix_4)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fror set up (min_df = .3,use_idf = True,sublinear_tf = True,max_df =0.6,max_features = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate term-document matrix with tf-idf scores\n",
    "tfidf_vectorizer_3 = TfidfVectorizer(min_df = .3,use_idf = True,sublinear_tf = True,max_df =0.6,max_features = 20000)\n",
    "tfidf_matrix_3 = tfidf_vectorizer_3.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000', '0000099', '15', '30', 'an', 'applications', 'are', 'as', 'at', 'august', 'by', 'co', 'continuing', 'div', 'division', 'from', 'has', 'have', 'it', 'july', 'may', 'nec', 'new', 'or', 'othr', 'project', 'science', 'sciences', 'standard', 'study', 'that', 'their', 'these', 'university', 'which']\n",
      "['project', 'science', 'sciences', 'standard', 'study', 'that', 'their', 'these', 'university', 'which']\n"
     ]
    }
   ],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer_3.get_feature_names()[:5120])\n",
    "print(tfidf_vectorizer_3.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer_3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79187\tstandard\n",
      "78064\tare\n",
      "76765\tby\n",
      "76560\tthat\n",
      "72314\tas\n",
      "71128\tapplications\n",
      "69766\tdivision\n",
      "67087\tuniversity\n",
      "66869\tsciences\n",
      "66028\tfrom\n",
      "65961\tan\n",
      "65876\tnec\n",
      "60811\tat\n",
      "60104\tthese\n",
      "60043\t0000099\n",
      "59888\tproject\n",
      "54503\twhich\n",
      "49021\tcontinuing\n",
      "47877\tdiv\n",
      "47127\t15\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer_3.inverse_transform(tfidf_matrix_3)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For set up  (min_df = .3,stop_words='english', use_idf = True,sublinear_tf = True,max_df =0.8,max_features = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate term-document matrix with tf-idf scores\n",
    "tfidf_vectorizer_2 = TfidfVectorizer(min_df = .3,stop_words='english', use_idf = True,sublinear_tf = True,max_df =0.8,max_features = 20000)\n",
    "tfidf_matrix_2 = tfidf_vectorizer_2.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000', '0000099', '15', '30', '31', 'applications', 'august', 'continuing', 'direct', 'div', 'division', 'edu', 'july', 'nec', 'new', 'othr', 'project', 'research', 'science', 'sciences', 'standard', 'study', 'university']\n",
      "['nec', 'new', 'othr', 'project', 'research', 'science', 'sciences', 'standard', 'study', 'university']\n"
     ]
    }
   ],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer_2.get_feature_names()[:5120])\n",
    "print(tfidf_vectorizer_2.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer_2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92379\tdirect\n",
      "86147\tresearch\n",
      "84619\tedu\n",
      "83008\t31\n",
      "79187\tstandard\n",
      "71128\tapplications\n",
      "69766\tdivision\n",
      "67087\tuniversity\n",
      "66869\tsciences\n",
      "65876\tnec\n",
      "60043\t0000099\n",
      "59888\tproject\n",
      "49021\tcontinuing\n",
      "47877\tdiv\n",
      "47127\t15\n",
      "46999\tnew\n",
      "46152\taugust\n",
      "44722\t30\n",
      "43074\tjuly\n",
      "42173\tscience\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer_2.inverse_transform(tfidf_matrix_2)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for set up : (min_df = .3,stop_words='english', use_idf = True,sublinear_tf = True,max_df =1,max_features = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate term-document matrix with tf-idf scores\n",
    "tfidf_vectorizer_1 = TfidfVectorizer(min_df = .3,stop_words='english', use_idf = True,sublinear_tf = True,max_df = 1.0,max_features = 20000)\n",
    "tfidf_matrix = tfidf_vectorizer_1.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<132041x51 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5066173 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000', '0000099', '15', '30', '31', 'abstract', 'amendment', 'amt', 'applications', 'applictn']\n",
      "['research', 'science', 'sciences', 'sponsor', 'standard', 'start', 'study', 'total', 'type', 'university']\n"
     ]
    }
   ],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer_1.get_feature_names()[:10])\n",
    "print(tfidf_vectorizer_1.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer_1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132041\tabstract\n",
      "132041\tapplictn\n",
      "132041\tfld\n",
      "132041\tprogram\n",
      "132041\tsponsor\n",
      "132041\tinvestigator\n",
      "132041\tamt\n",
      "132041\ttotal\n",
      "132041\texpected\n",
      "132041\texpires\n",
      "132041\tstart\n",
      "132041\tmanager\n",
      "132041\tprgm\n",
      "132041\tinstr\n",
      "132041\tnumber\n",
      "132041\tfile\n",
      "132041\tdate\n",
      "132041\tamendment\n",
      "132041\tlatest\n",
      "132041\torg\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer_1.inverse_transform(tfidf_matrix)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### form the above results for the features it can be seen that the best set up among the proposed one is  the last set up or (min_df = .3,stop_words='english', use_idf = True,sublinear_tf = True,max_df =1,max_features = 20000)). When the min_df is chosen to be 0 or very low many of the feautres are just numbres so with applying the restriction of the 0.3 for the min_df many of the features which are not meaningful looks to be removed. Also with removing the stop words many mainly un importatnt features such as 'an' , 'and' ,...... also get removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### With the last set up we do the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a subset of the documents in clustering for faster calculation and easier interpretation of results\n",
    "matrix_sample = tfidf_matrix[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=30, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=123, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Do clustering\n",
    "km = KMeans(n_clusters=30, random_state=123, verbose=0)\n",
    "km.fit(matrix_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 29, 23, 14, 21,  0,  9, 24, 28, 16, 12,  4, 23,  6,  9, 18, 16,\n",
       "       29, 18, 12, 29, 25, 11, 21, 22, 12, 20, 20, 14, 18,  1,  9,  7, 13,\n",
       "       18, 12, 22,  4,  4, 12, 12, 28, 12, 26, 18, 18, 12,  9, 18, 12, 21,\n",
       "       15, 12, 21,  9,  4, 12, 27, 16, 13,  1, 27,  9, 21, 16, 12, 12, 12,\n",
       "        6, 17, 11,  9,  2,  5, 16, 24, 15, 20, 18, 20, 22, 14, 12, 22, 14,\n",
       "       21, 16, 16, 26, 25, 13, 21, 29, 14,  9, 18,  4, 16, 16, 15, 16, 13,\n",
       "        5, 25, 14,  8,  6, 29, 10, 15,  7, 21, 17, 17, 13,  1, 15, 15, 17,\n",
       "       19,  8, 29, 12, 15, 21,  5, 28, 17,  6, 13, 24, 25, 18, 18, 13, 19,\n",
       "       13, 22,  7, 19, 18,  3, 16, 16,  9, 11, 16, 12, 12, 18, 12, 21, 12,\n",
       "       21, 11, 28, 12,  5, 22, 11, 25, 13,  9, 20, 23, 28, 21, 11, 18, 26,\n",
       "       24,  7, 14,  9, 18,  1, 23, 29,  6, 13,  7, 15, 14,  3, 19, 22, 14,\n",
       "       13, 27,  3, 27,  2, 14, 17, 25, 12, 23,  4, 14,  8, 14, 20, 25,  7,\n",
       "        1,  8, 22,  0, 22, 28, 16,  2,  2, 12, 18,  5, 29, 14, 23,  4, 20,\n",
       "        5, 13,  5, 22,  5,  2,  8, 24, 23,  5,  4, 15,  0, 13,  2,  8,  8,\n",
       "       25,  8, 22,  6, 19,  0,  1, 13,  5, 18,  5,  6, 19, 13, 13,  1,  2,\n",
       "        1,  5,  2,  6, 18,  8,  0,  5,  5,  8,  8,  6,  4,  5,  8,  5,  5,\n",
       "        8,  5,  8, 24,  8,  8,  8,  5,  4,  5,  9,  2, 25,  9,  1,  2, 25,\n",
       "        5,  5,  8,  8,  9, 27, 19, 15,  8,  8,  5,  2,  6,  0, 28, 19,  5,\n",
       "        8,  5,  2,  5,  6,  8,  8, 27,  2,  5, 19, 12,  1,  5,  2,  2, 19,\n",
       "        2,  8,  6,  2,  5,  2,  8,  0,  8, 15,  6,  1,  6,  4,  8, 25,  2,\n",
       "        5, 27, 13, 17, 18, 21,  6,  6, 14, 24, 19, 15, 29,  6,  6,  0, 13,\n",
       "        0,  1, 25, 29, 17, 12, 24, 12,  6,  6, 13,  8,  3,  8,  1,  5,  2,\n",
       "        5, 18, 18, 13,  7, 13, 20, 13, 12,  1, 15,  3,  1, 18, 22,  7,  6,\n",
       "        6, 24, 13, 14, 18, 29, 23,  1, 22,  3, 28,  6,  7,  2, 22,  2, 23,\n",
       "       25, 22,  3, 23, 28,  5, 21,  3, 11, 21, 23,  7,  0, 14, 22,  4, 26,\n",
       "       27,  3, 22,  2,  9, 21, 13,  5,  3,  1, 18,  6, 19,  7,  1,  0,  1,\n",
       "       24, 14, 22, 25,  8, 27,  7,  3, 13, 27, 22, 15, 13,  8, 17, 26, 14,\n",
       "       29,  6,  6, 26, 17, 13, 10, 26, 20,  7, 23, 29, 26, 13, 29,  3, 14,\n",
       "       13,  6, 16, 21, 23, 18, 23, 28, 14,  7,  4, 26,  4, 22, 23,  4,  3,\n",
       "       22,  6, 23, 18, 21,  2, 21, 21,  3,  4,  5,  5, 28,  9, 13,  1, 26,\n",
       "       23, 27, 19, 19, 26, 11,  4, 10,  4,  4, 14, 13, 10, 13, 24,  7,  3,\n",
       "        4, 16,  7, 18, 19, 20,  7,  7, 22,  4,  4, 15,  7, 17, 25, 20,  3,\n",
       "       12, 12,  3, 25, 13, 21, 25,  0, 20,  7,  4, 25, 23, 13, 21, 20, 13,\n",
       "        6,  4, 22,  8,  5, 23, 11, 19, 14,  9, 17, 28, 11, 21, 25, 27, 28,\n",
       "        5,  4, 13, 11, 18, 10, 10, 21, 26, 17, 27, 22, 22, 21, 19,  6, 26,\n",
       "       21,  3, 15,  7, 28, 20, 14, 21, 22, 14, 29, 13,  9, 29,  9,  5,  0,\n",
       "        8,  4, 22,  7, 10, 10, 19,  7, 27, 16, 17, 22, 14,  2, 10, 28,  7,\n",
       "        5, 23,  5, 22, 22, 26, 21,  7, 24, 28,  0, 26,  7, 23, 27, 19,  6,\n",
       "       22, 28, 24,  7,  4, 22, 25,  1,  1,  4, 23, 22,  9, 20, 22,  1,  4,\n",
       "       13, 13,  8, 12, 14, 22, 29, 20, 10,  9, 11,  2, 20, 13, 28, 11,  7,\n",
       "       29, 20, 23,  7, 23,  1, 13, 13,  0,  7, 22,  5, 24, 21, 10, 17,  1,\n",
       "       17, 17, 21, 14,  6, 25,  4, 20, 22, 13, 10, 20, 27, 28, 15, 22, 13,\n",
       "       23,  8, 29, 22, 22,  4,  1, 17, 16, 10,  1, 21, 20, 29, 25, 14, 24,\n",
       "       17,  7, 22, 28,  7, 21,  9, 16,  7,  9,  6, 21, 29, 13, 24, 24, 24,\n",
       "       24, 26, 24, 11, 28, 19, 11,  4, 19, 11, 11, 23, 29,  6,  8, 14, 22,\n",
       "       26, 17,  6, 22,  7, 21, 29, 19,  2, 19, 20, 22,  6, 17, 11,  6, 11,\n",
       "        2, 14, 15, 19, 17, 29,  1, 20,  1, 13,  3,  3, 19, 25,  8, 19, 10,\n",
       "       13, 23,  4, 29, 14, 13,  5, 28, 10, 11, 17, 17, 11, 18, 14, 20, 24,\n",
       "       24, 24, 27,  7,  4, 17,  2, 14, 21, 17, 26,  2, 29,  7,  8, 13,  9,\n",
       "        4,  4, 22, 15, 29,  7, 29, 10, 10, 14, 20, 27,  0,  2, 29,  9,  4,\n",
       "       19, 10, 10, 10, 10, 10,  4, 17, 20,  5,  1, 23, 12,  7, 23, 15, 29,\n",
       "       29,  6, 23, 14, 23, 14, 29, 19,  6, 29, 18, 19,  4, 25, 20, 22, 23,\n",
       "        9, 25, 14,  1, 23,  1, 14, 22,  4, 23, 29,  8, 22, 13,  4,  6, 14,\n",
       "       13, 18, 13, 17, 20, 14, 17,  6, 17,  8, 13,  7, 16, 16, 16, 16, 18,\n",
       "       18, 18, 18, 16, 16,  6, 27,  3, 27, 28, 23, 10, 23, 14, 25,  6, 22,\n",
       "       14,  9,  6,  4, 17, 26, 13, 13, 13, 27, 25,  0, 13,  4,  9, 13,  2,\n",
       "       13,  9, 27, 13,  5,  4, 13, 17, 17, 14, 23,  4,  7, 26,  0, 13, 23,\n",
       "        1, 26, 26, 14, 13, 18, 18, 11,  8, 29,  6, 24, 24, 24, 24, 24,  1,\n",
       "        0, 18,  1, 11, 22,  1, 14, 14,  6,  0, 22, 14, 13,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (20 docs)\n",
      "new, study, award, research, 30, science, 0000099, program, investigator, nec\n",
      "\n",
      "Cluster: 1 (37 docs)\n",
      "award, 0000099, investigator, nec, 15, estimated, date, program, research, project\n",
      "\n",
      "Cluster: 2 (32 docs)\n",
      "research, award, science, program, 30, div, investigator, 0000099, 15, nec\n",
      "\n",
      "Cluster: 3 (21 docs)\n",
      "august, award, investigator, estimated, date, 15, nsf, sciences, program, study\n",
      "\n",
      "Cluster: 4 (45 docs)\n",
      "award, continuing, 0000099, investigator, nec, estimated, study, project, date, principal\n",
      "\n",
      "Cluster: 5 (45 docs)\n",
      "research, award, program, investigator, nsf, estimated, date, university, project, current\n",
      "\n",
      "Cluster: 6 (46 docs)\n",
      "science, award, 0000099, nec, investigator, program, university, research, project, estimated\n",
      "\n",
      "Cluster: 7 (41 docs)\n",
      "study, award, 30, continuing, 0000099, investigator, nec, july, estimated, principal\n",
      "\n",
      "Cluster: 8 (42 docs)\n",
      "research, university, award, 0000099, investigator, continuing, nec, estimated, date, program\n",
      "\n",
      "Cluster: 9 (29 docs)\n",
      "science, research, award, 30, university, estimated, investigator, date, nsf, program\n",
      "\n",
      "Cluster: 10 (23 docs)\n",
      "august, award, july, investigator, estimated, science, date, program, 0000099, nec\n",
      "\n",
      "Cluster: 11 (23 docs)\n",
      "award, div, science, july, investigator, estimated, date, 15, nsf, continuing\n",
      "\n",
      "Cluster: 12 (31 docs)\n",
      "award, 30, sciences, investigator, continuing, project, estimated, 15, date, program\n",
      "\n",
      "Cluster: 13 (64 docs)\n",
      "award, 30, 0000099, nec, estimated, investigator, date, project, 15, continuing\n",
      "\n",
      "Cluster: 14 (47 docs)\n",
      "award, div, investigator, date, estimated, nsf, program, research, continuing, august\n",
      "\n",
      "Cluster: 15 (20 docs)\n",
      "study, award, university, 30, project, continuing, research, estimated, program, 0000099\n",
      "\n",
      "Cluster: 16 (25 docs)\n",
      "award, sciences, investigator, estimated, nsf, date, 30, program, july, principal\n",
      "\n",
      "Cluster: 17 (33 docs)\n",
      "science, award, program, estimated, investigator, date, nsf, 15, standard, 30\n",
      "\n",
      "Cluster: 18 (38 docs)\n",
      "award, continuing, sciences, investigator, estimated, date, program, nsf, study, july\n",
      "\n",
      "Cluster: 19 (29 docs)\n",
      "research, science, award, div, investigator, estimated, 15, date, program, university\n",
      "\n",
      "Cluster: 20 (28 docs)\n",
      "new, award, august, div, estimated, date, research, continuing, project, nec\n",
      "\n",
      "Cluster: 21 (35 docs)\n",
      "study, award, estimated, investigator, date, program, nsf, sciences, 15, project\n",
      "\n",
      "Cluster: 22 (51 docs)\n",
      "study, award, 0000099, investigator, nec, estimated, july, project, date, principal\n",
      "\n",
      "Cluster: 23 (38 docs)\n",
      "july, award, 0000099, investigator, nec, estimated, date, research, continuing, nsf\n",
      "\n",
      "Cluster: 24 (28 docs)\n",
      "research, new, university, award, program, investigator, nsf, estimated, date, july\n",
      "\n",
      "Cluster: 25 (27 docs)\n",
      "new, award, program, 0000099, continuing, nec, estimated, investigator, date, study\n",
      "\n",
      "Cluster: 26 (22 docs)\n",
      "science, award, project, study, research, university, 15, estimated, investigator, date\n",
      "\n",
      "Cluster: 27 (22 docs)\n",
      "research, july, award, study, estimated, continuing, date, investigator, 15, program\n",
      "\n",
      "Cluster: 28 (23 docs)\n",
      "research, award, continuing, investigator, 30, estimated, date, nsf, program, current\n",
      "\n",
      "Cluster: 29 (35 docs)\n",
      "science, award, div, august, 0000099, 15, 31, july, project, research\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_clusters = {'Cluster0':['(20 docs)', 'new', 'study', 'award', 'research', '30', 'science', '0000099', 'program', 'investigator', 'nec'],\n",
    "                                 \"Cluster1\": ['(37 docs)','award', '0000099', 'investigator', 'nec', '15', 'estimated', 'date', 'program', 'research', 'project'],\n",
    "                                 'Cluster2': ['(32 docs)','research', 'award', 'science', 'program', '30', 'diCluster2v', 'investigator', '0000099', '15', 'nec'],\n",
    "                                 'Cluster3':['(21 docs)','august', 'award', 'investigator', 'estimated', 'date', '15', 'nsf', 'sciences', 'program', 'study'],\n",
    "                    'Cluster4':['(45 docs)','award', 'continuing', '0000099', 'investigator', 'nec', 'estimated', 'study', 'project', 'date', 'principal'],\n",
    "                    'Cluster5':['(45 docs)','research', 'award', 'program', 'investigator', 'nsf', 'estimated', 'date', 'university', 'project', 'current'],\n",
    "                    'Cluster6':['(46 docs)','science', 'award', '0000099', 'nec', 'investigator', 'program', 'university', 'research', 'project', 'estimated'],\n",
    "                    'Cluster7' :['(41 docs)', 'study', 'award', '30', 'continuing', '0000099', 'investigator', 'nec', 'july', 'estimated', 'principal'],\n",
    "                    'Cluster8 ': ['(42 docs)', 'research', 'university', 'award', '0000099', 'investigator', 'continuing', 'nec', 'estimated', 'date', 'program'],\n",
    "                    'Cluster9': ['(29 docs)', 'science', 'research', 'award', '30', 'university', 'estimated', 'investigator', 'date', 'nsf', 'program'],\n",
    "                    'Cluster10': ['(23 docs)','august', 'award', 'july', 'investigator', 'estimated', 'science', 'date', 'program', '0000099', 'nec']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Cluster0</td>\n",
       "      <td>(20 docs)</td>\n",
       "      <td>new</td>\n",
       "      <td>study</td>\n",
       "      <td>award</td>\n",
       "      <td>research</td>\n",
       "      <td>30</td>\n",
       "      <td>science</td>\n",
       "      <td>0000099</td>\n",
       "      <td>program</td>\n",
       "      <td>investigator</td>\n",
       "      <td>nec</td>\n",
       "      <td>new study award</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster1</td>\n",
       "      <td>(37 docs)</td>\n",
       "      <td>award</td>\n",
       "      <td>0000099</td>\n",
       "      <td>investigator</td>\n",
       "      <td>nec</td>\n",
       "      <td>15</td>\n",
       "      <td>estimated</td>\n",
       "      <td>date</td>\n",
       "      <td>program</td>\n",
       "      <td>research</td>\n",
       "      <td>project</td>\n",
       "      <td>research program date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster2</td>\n",
       "      <td>(32 docs)</td>\n",
       "      <td>research</td>\n",
       "      <td>award</td>\n",
       "      <td>science</td>\n",
       "      <td>program</td>\n",
       "      <td>30</td>\n",
       "      <td>diCluster2v</td>\n",
       "      <td>investigator</td>\n",
       "      <td>0000099</td>\n",
       "      <td>15</td>\n",
       "      <td>nec</td>\n",
       "      <td>science program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster3</td>\n",
       "      <td>(21 docs)</td>\n",
       "      <td>august</td>\n",
       "      <td>award</td>\n",
       "      <td>investigator</td>\n",
       "      <td>estimated</td>\n",
       "      <td>date</td>\n",
       "      <td>15</td>\n",
       "      <td>nsf</td>\n",
       "      <td>sciences</td>\n",
       "      <td>program</td>\n",
       "      <td>study</td>\n",
       "      <td>research program-Augest-date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster4</td>\n",
       "      <td>(45 docs)</td>\n",
       "      <td>award</td>\n",
       "      <td>continuing</td>\n",
       "      <td>0000099</td>\n",
       "      <td>investigator</td>\n",
       "      <td>nec</td>\n",
       "      <td>estimated</td>\n",
       "      <td>study</td>\n",
       "      <td>project</td>\n",
       "      <td>date</td>\n",
       "      <td>principal</td>\n",
       "      <td>continuing study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster5</td>\n",
       "      <td>(45 docs)</td>\n",
       "      <td>research</td>\n",
       "      <td>award</td>\n",
       "      <td>program</td>\n",
       "      <td>investigator</td>\n",
       "      <td>nsf</td>\n",
       "      <td>estimated</td>\n",
       "      <td>date</td>\n",
       "      <td>university</td>\n",
       "      <td>project</td>\n",
       "      <td>current</td>\n",
       "      <td>current project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster6</td>\n",
       "      <td>(46 docs)</td>\n",
       "      <td>science</td>\n",
       "      <td>award</td>\n",
       "      <td>0000099</td>\n",
       "      <td>nec</td>\n",
       "      <td>investigator</td>\n",
       "      <td>program</td>\n",
       "      <td>university</td>\n",
       "      <td>research</td>\n",
       "      <td>project</td>\n",
       "      <td>estimated</td>\n",
       "      <td>university project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster7</td>\n",
       "      <td>(41 docs)</td>\n",
       "      <td>study</td>\n",
       "      <td>award</td>\n",
       "      <td>30</td>\n",
       "      <td>continuing</td>\n",
       "      <td>0000099</td>\n",
       "      <td>investigator</td>\n",
       "      <td>nec</td>\n",
       "      <td>july</td>\n",
       "      <td>estimated</td>\n",
       "      <td>principal</td>\n",
       "      <td>principal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster8</td>\n",
       "      <td>(42 docs)</td>\n",
       "      <td>research</td>\n",
       "      <td>university</td>\n",
       "      <td>award</td>\n",
       "      <td>0000099</td>\n",
       "      <td>investigator</td>\n",
       "      <td>continuing</td>\n",
       "      <td>nec</td>\n",
       "      <td>estimated</td>\n",
       "      <td>date</td>\n",
       "      <td>program</td>\n",
       "      <td>research-university</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster9</td>\n",
       "      <td>(29 docs)</td>\n",
       "      <td>science</td>\n",
       "      <td>research</td>\n",
       "      <td>award</td>\n",
       "      <td>30</td>\n",
       "      <td>university</td>\n",
       "      <td>estimated</td>\n",
       "      <td>investigator</td>\n",
       "      <td>date</td>\n",
       "      <td>nsf</td>\n",
       "      <td>program</td>\n",
       "      <td>science award</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cluster10</td>\n",
       "      <td>(23 docs)</td>\n",
       "      <td>august</td>\n",
       "      <td>award</td>\n",
       "      <td>july</td>\n",
       "      <td>investigator</td>\n",
       "      <td>estimated</td>\n",
       "      <td>science</td>\n",
       "      <td>date</td>\n",
       "      <td>program</td>\n",
       "      <td>0000099</td>\n",
       "      <td>nec</td>\n",
       "      <td>program data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1           2             3             4  \\\n",
       "Cluster0   (20 docs)       new       study         award      research   \n",
       "Cluster1   (37 docs)     award     0000099  investigator           nec   \n",
       "Cluster2   (32 docs)  research       award       science       program   \n",
       "Cluster3   (21 docs)    august       award  investigator     estimated   \n",
       "Cluster4   (45 docs)     award  continuing       0000099  investigator   \n",
       "Cluster5   (45 docs)  research       award       program  investigator   \n",
       "Cluster6   (46 docs)   science       award       0000099           nec   \n",
       "Cluster7   (41 docs)     study       award            30    continuing   \n",
       "Cluster8   (42 docs)  research  university         award       0000099   \n",
       "Cluster9   (29 docs)   science    research         award            30   \n",
       "Cluster10  (23 docs)    august       award          july  investigator   \n",
       "\n",
       "                      5             6             7           8             9  \\\n",
       "Cluster0             30       science       0000099     program  investigator   \n",
       "Cluster1             15     estimated          date     program      research   \n",
       "Cluster2             30   diCluster2v  investigator     0000099            15   \n",
       "Cluster3           date            15           nsf    sciences       program   \n",
       "Cluster4            nec     estimated         study     project          date   \n",
       "Cluster5            nsf     estimated          date  university       project   \n",
       "Cluster6   investigator       program    university    research       project   \n",
       "Cluster7        0000099  investigator           nec        july     estimated   \n",
       "Cluster8   investigator    continuing           nec   estimated          date   \n",
       "Cluster9     university     estimated  investigator        date           nsf   \n",
       "Cluster10     estimated       science          date     program       0000099   \n",
       "\n",
       "                  10                          label  \n",
       "Cluster0         nec                new study award  \n",
       "Cluster1     project          research program date  \n",
       "Cluster2         nec                science program  \n",
       "Cluster3       study   research program-Augest-date  \n",
       "Cluster4   principal               continuing study  \n",
       "Cluster5     current                current project  \n",
       "Cluster6   estimated             university project  \n",
       "Cluster7   principal                      principal  \n",
       "Cluster8     program           research-university   \n",
       "Cluster9     program                  science award  \n",
       "Cluster10        nec                   program data  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "frame = pd.DataFrame.from_dict(first_clusters, orient='index')\n",
    "frame['label'] = ['new study award', 'research program date', ' science program', ' research program-Augest-date',\n",
    "                  'continuing study', ' current project', 'university project', ' principal', ' research-university ', \n",
    "                 'science award', 'program data']\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### almost all the culsters include same words , and all can be labled as reasech and award. They all look alike and I v=can not choose any the best or the worse cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hierarchical clustering (alternative approach)\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "Z = linkage(matrix_sample.todense(), metric='cosine', method='complete')\n",
    "_ = dendrogram(Z, no_labels=True) # Plot dentrogram chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# View dendrogram for subset\n",
    "Z_ = linkage(matrix_sample.todense()[:25], metric='cosine', method='complete')\n",
    "_ = dendrogram(Z_, no_labels=True) # Plot dentrogram chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get flat clusters from cluster hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 1 (1000 docs)\n",
      "award, investigator, estimated, research, date, program, study, nsf, science, continuing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clusters = fcluster(Z, 0.99, criterion='distance') # Create flat clusters by distance threshold\n",
    "\n",
    "print_clusters(matrix_sample, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As it can be seen from the above with distance of 0.99 all the documents can be categorised in one cluster which can be ;abeled as research study award ,  however if the distance decarese at the distance of 0.5 this inludes more clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 1 (330 docs)\n",
      "award, 0000099, study, nec, investigator, estimated, continuing, 30, date, project\n",
      "\n",
      "Cluster: 2 (149 docs)\n",
      "award, investigator, estimated, date, sciences, nsf, july, program, continuing, august\n",
      "\n",
      "Cluster: 3 (76 docs)\n",
      "study, award, project, estimated, investigator, date, program, research, nsf, 15\n",
      "\n",
      "Cluster: 4 (33 docs)\n",
      "new, award, research, science, investigator, div, program, 15, project, 30\n",
      "\n",
      "Cluster: 5 (37 docs)\n",
      "award, div, investigator, date, estimated, nsf, program, 30, continuing, july\n",
      "\n",
      "Cluster: 6 (175 docs)\n",
      "research, award, investigator, university, estimated, program, date, nsf, continuing, 30\n",
      "\n",
      "Cluster: 7 (200 docs)\n",
      "science, award, research, investigator, estimated, program, date, 15, div, nsf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clusters = fcluster(Z, 0.5, criterion='distance') # Create flat clusters by distance threshold\n",
    "\n",
    "print_clusters(matrix_sample, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### despite the fact that now there are differnt clusters they do not show any meaningful and conceptual difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 1 (96 docs)\n",
      "study, award, 0000099, investigator, nec, estimated, continuing, july, date, project\n",
      "\n",
      "Cluster: 2 (43 docs)\n",
      "new, award, study, 0000099, investigator, continuing, nec, estimated, program, university\n",
      "\n",
      "Cluster: 3 (13 docs)\n",
      "july, august, award, 15, science, 0000099, investigator, new, nec, principal\n",
      "\n",
      "Cluster: 4 (53 docs)\n",
      "award, continuing, 0000099, investigator, nec, estimated, research, date, july, project\n",
      "\n",
      "Cluster: 5 (5 docs)\n",
      "continuing, othr, 0000, award, new, investigator, research, program, date, project\n",
      "\n",
      "Cluster: 6 (66 docs)\n",
      "award, 30, 0000099, nec, investigator, estimated, continuing, date, project, program\n",
      "\n",
      "Cluster: 7 (54 docs)\n",
      "award, 0000099, nec, investigator, research, estimated, 15, program, date, project\n",
      "\n",
      "Cluster: 8 (37 docs)\n",
      "august, award, estimated, investigator, date, research, nsf, july, div, new\n",
      "\n",
      "Cluster: 9 (37 docs)\n",
      "july, award, investigator, estimated, date, nsf, 30, program, nec, 0000099\n",
      "\n",
      "Cluster: 10 (24 docs)\n",
      "award, sciences, 15, investigator, program, estimated, date, nsf, 30, continuing\n",
      "\n",
      "Cluster: 11 (51 docs)\n",
      "award, sciences, investigator, continuing, estimated, date, program, project, nsf, 30\n",
      "\n",
      "Cluster: 12 (36 docs)\n",
      "study, award, investigator, estimated, date, program, nsf, sciences, current, principal\n",
      "\n",
      "Cluster: 13 (15 docs)\n",
      "study, project, new, award, continuing, university, research, program, estimated, investigator\n",
      "\n",
      "Cluster: 14 (8 docs)\n",
      "project, study, award, estimated, date, investigator, program, nsf, research, div\n",
      "\n",
      "Cluster: 15 (17 docs)\n",
      "project, research, award, program, estimated, 15, date, science, nsf, investigator\n",
      "\n",
      "Cluster: 16 (15 docs)\n",
      "new, science, research, award, project, university, program, investigator, august, nsf\n",
      "\n",
      "Cluster: 17 (18 docs)\n",
      "new, award, research, div, investigator, study, program, 30, 15, principal\n",
      "\n",
      "Cluster: 18 (9 docs)\n",
      "july, award, div, new, estimated, date, investigator, 30, continuing, nsf\n",
      "\n",
      "Cluster: 19 (28 docs)\n",
      "award, div, investigator, date, estimated, nsf, program, 30, continuing, current\n",
      "\n",
      "Cluster: 20 (86 docs)\n",
      "research, award, investigator, estimated, continuing, university, date, nsf, 30, program\n",
      "\n",
      "Cluster: 21 (49 docs)\n",
      "research, university, award, program, investigator, estimated, date, nsf, new, principal\n",
      "\n",
      "Cluster: 22 (40 docs)\n",
      "research, award, program, estimated, date, investigator, nsf, july, university, new\n",
      "\n",
      "Cluster: 23 (12 docs)\n",
      "science, university, study, award, research, estimated, investigator, 15, program, date\n",
      "\n",
      "Cluster: 24 (24 docs)\n",
      "science, award, estimated, date, investigator, program, nsf, 15, standard, direct\n",
      "\n",
      "Cluster: 25 (16 docs)\n",
      "science, project, award, research, 0000099, nec, estimated, nsf, program, investigator\n",
      "\n",
      "Cluster: 26 (46 docs)\n",
      "science, research, award, university, investigator, 30, div, estimated, current, 15\n",
      "\n",
      "Cluster: 27 (12 docs)\n",
      "science, award, research, sciences, continuing, program, 30, 15, estimated, div\n",
      "\n",
      "Cluster: 28 (14 docs)\n",
      "august, science, award, research, july, 31, div, estimated, project, program\n",
      "\n",
      "Cluster: 29 (25 docs)\n",
      "science, award, div, investigator, continuing, august, estimated, research, date, study\n",
      "\n",
      "Cluster: 30 (51 docs)\n",
      "science, award, 0000099, investigator, nec, estimated, date, program, july, 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clusters = fcluster(Z, 30, criterion='maxclust') # Create fix number of flat clusters\n",
    "print_clusters(matrix_sample, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in the above, clustering is done with hierarchy, and with comparision with k-neighboring there is no conceptual difference for the two applied methodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic modeling demo\n",
    "# Fast and simple tokenization\n",
    "new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in document]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0584\tthe\n",
      "0.0530\tof\n",
      "0.0330\tand\n",
      "0.0276\tto\n",
      "0.0233\tin\n",
      "0.0127\tis\n",
      "0.0109\tthat\n",
      "0.0096\twill\n",
      "0.0091\tThe\n",
      "0.0080\ton\n",
      "\n",
      "Topic 1\n",
      "0.0281\tof\n",
      "0.0221\tAward\n",
      "0.0163\tInvestigator\n",
      "0.0161\tthe\n",
      "0.0150\tProgram\n",
      "0.0148\tNSF\n",
      "0.0146\tDate\n",
      "0.0146\tEstimated\n",
      "0.0096\tMATHEMATICAL\n",
      "0.0090\tPrincipal\n",
      "\n",
      "Topic 2\n",
      "0.0478\tthe\n",
      "0.0377\tof\n",
      "0.0320\tand\n",
      "0.0159\tto\n",
      "0.0148\tin\n",
      "0.0119\twill\n",
      "0.0118\tAward\n",
      "0.0100\tInvestigator\n",
      "0.0084\tProgram\n",
      "0.0082\tNSF\n",
      "\n",
      "Topic 3\n",
      "0.0613\tNC\n",
      "0.0361\t919\n",
      "0.0331\tHill\n",
      "0.0281\tCarolina\n",
      "0.0265\tChapel\n",
      "0.0170\tNorth\n",
      "0.0114\t27514\n",
      "0.0106\tunc\n",
      "0.0090\t962\n",
      "0.0074\tBynum\n",
      "\n",
      "Topic 4\n",
      "0.0407\tand\n",
      "0.0326\tthe\n",
      "0.0252\tof\n",
      "0.0187\tto\n",
      "0.0183\tin\n",
      "0.0129\tInvestigator\n",
      "0.0126\twill\n",
      "0.0110\tAward\n",
      "0.0108\tfor\n",
      "0.0102\tstudents\n",
      "\n",
      "Topic 5\n",
      "0.0246\tRutgers\n",
      "0.0234\tNew\n",
      "0.0191\tBrunswick\n",
      "0.0169\tAmherst\n",
      "0.0155\tYale\n",
      "0.0148\tPlaza\n",
      "0.0141\t408\n",
      "0.0127\tIII\n",
      "0.0124\tCruz\n",
      "0.0121\tSanta\n",
      "\n",
      "Topic 6\n",
      "0.0420\tof\n",
      "0.0399\tthe\n",
      "0.0318\tand\n",
      "0.0182\tto\n",
      "0.0155\tin\n",
      "0.0140\twill\n",
      "0.0112\tbe\n",
      "0.0099\tThe\n",
      "0.0098\tAward\n",
      "0.0090\tfor\n",
      "\n",
      "Topic 7\n",
      "0.0287\tand\n",
      "0.0223\tof\n",
      "0.0195\tthe\n",
      "0.0192\tto\n",
      "0.0153\tfor\n",
      "0.0115\tAward\n",
      "0.0101\tInvestigator\n",
      "0.0085\tThe\n",
      "0.0079\tis\n",
      "0.0078\tNSF\n",
      "\n",
      "Topic 8\n",
      "0.0320\tspeech\n",
      "0.0188\tChampaign\n",
      "0.0186\tNMR\n",
      "0.0173\tlanguage\n",
      "0.0158\tSanta\n",
      "0.0142\tPittsburgh\n",
      "0.0142\tBarbara\n",
      "0.0107\tUrbana\n",
      "0.0106\tWright\n",
      "0.0104\tIrvine\n",
      "\n",
      "Topic 9\n",
      "0.0451\tthe\n",
      "0.0446\tof\n",
      "0.0276\tand\n",
      "0.0223\tin\n",
      "0.0210\tto\n",
      "0.0118\twill\n",
      "0.0104\tis\n",
      "0.0092\tthat\n",
      "0.0087\tbe\n",
      "0.0082\tAward\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in lda_model.show_topics(num_words=10, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    for term, score in topic:\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above topics are not meaningful and includs many articles,auxilary verbs and so on, so we exlude these sore of words in the following coman lines  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mitochondrial',\n",
       " 'DNA',\n",
       " 'and',\n",
       " 'Historical',\n",
       " 'Demography',\n",
       " 'Type',\n",
       " 'Award',\n",
       " 'NSF',\n",
       " 'Org',\n",
       " 'DEB',\n",
       " 'Latest',\n",
       " 'Amendment',\n",
       " 'Date',\n",
       " 'August',\n",
       " '1991',\n",
       " 'File',\n",
       " 'a9000006',\n",
       " 'Award',\n",
       " 'Number',\n",
       " '9000006',\n",
       " 'Award',\n",
       " 'Instr',\n",
       " 'Continuing',\n",
       " 'grant',\n",
       " 'Prgm',\n",
       " 'Manager',\n",
       " 'Scott',\n",
       " 'Collins',\n",
       " 'DEB',\n",
       " 'DIVISION',\n",
       " 'OF',\n",
       " 'ENVIRONMENTAL',\n",
       " 'BIOLOGY',\n",
       " 'BIO',\n",
       " 'DIRECT',\n",
       " 'FOR',\n",
       " 'BIOLOGICAL',\n",
       " 'SCIENCES',\n",
       " 'Start',\n",
       " 'Date',\n",
       " 'June',\n",
       " '1990',\n",
       " 'Expires',\n",
       " 'November',\n",
       " '30',\n",
       " '1992',\n",
       " 'Estimated',\n",
       " 'Expected',\n",
       " 'Total',\n",
       " 'Amt',\n",
       " '179720',\n",
       " 'Estimated',\n",
       " 'Investigator',\n",
       " 'Stephen',\n",
       " 'Palumbi',\n",
       " 'Principal',\n",
       " 'Investigator',\n",
       " 'current',\n",
       " 'Sponsor',\n",
       " 'of',\n",
       " 'Hawaii',\n",
       " 'Manoa',\n",
       " '2530',\n",
       " 'Dole',\n",
       " 'Street',\n",
       " 'Honolulu',\n",
       " 'HI',\n",
       " '968222225',\n",
       " '808',\n",
       " '956',\n",
       " '7800',\n",
       " 'NSF',\n",
       " 'Program',\n",
       " '1127',\n",
       " 'SYSTEMATIC',\n",
       " 'POPULATION',\n",
       " 'BIOLO',\n",
       " 'Fld',\n",
       " 'Applictn',\n",
       " '0000099',\n",
       " 'Other',\n",
       " 'Applications',\n",
       " 'NEC',\n",
       " '61',\n",
       " 'Life',\n",
       " 'Science',\n",
       " 'Biological',\n",
       " 'Program',\n",
       " 'Ref',\n",
       " '9285',\n",
       " 'Abstract',\n",
       " 'Commercial',\n",
       " 'exploitation',\n",
       " 'over',\n",
       " 'the',\n",
       " 'past',\n",
       " 'two',\n",
       " 'hundred',\n",
       " 'years',\n",
       " 'drove',\n",
       " 'the',\n",
       " 'great',\n",
       " 'Mysticete',\n",
       " 'whales',\n",
       " 'to',\n",
       " 'near',\n",
       " 'extinction',\n",
       " 'Variation',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sizes',\n",
       " 'of',\n",
       " 'populations',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'exploitation',\n",
       " 'minimal',\n",
       " 'population',\n",
       " 'size',\n",
       " 'during',\n",
       " 'exploitation',\n",
       " 'and',\n",
       " 'current',\n",
       " 'population',\n",
       " 'sizes',\n",
       " 'permit',\n",
       " 'analyses',\n",
       " 'of',\n",
       " 'the',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'differing',\n",
       " 'levels',\n",
       " 'of',\n",
       " 'exploitation',\n",
       " 'on',\n",
       " 'species',\n",
       " 'with',\n",
       " 'different',\n",
       " 'biogeographical',\n",
       " 'distributions',\n",
       " 'and',\n",
       " 'life',\n",
       " 'history',\n",
       " 'characteristics',\n",
       " 'Dr',\n",
       " 'Stephen',\n",
       " 'Palumbi',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Hawaii',\n",
       " 'will',\n",
       " 'study',\n",
       " 'the',\n",
       " 'genetic',\n",
       " 'population',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'three',\n",
       " 'whale',\n",
       " 'species',\n",
       " 'in',\n",
       " 'this',\n",
       " 'context',\n",
       " 'the',\n",
       " 'Humpback',\n",
       " 'Whale',\n",
       " 'the',\n",
       " 'Gray',\n",
       " 'Whale',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Bowhead',\n",
       " 'Whale',\n",
       " 'The',\n",
       " 'effect',\n",
       " 'of',\n",
       " 'demographic',\n",
       " 'history',\n",
       " 'will',\n",
       " 'be',\n",
       " 'determined',\n",
       " 'by',\n",
       " 'comparing',\n",
       " 'the',\n",
       " 'genetic',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'three',\n",
       " 'species',\n",
       " 'Additional',\n",
       " 'studies',\n",
       " 'will',\n",
       " 'be',\n",
       " 'carried',\n",
       " 'out',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Humpback',\n",
       " 'Whale',\n",
       " 'The',\n",
       " 'humpback',\n",
       " 'has',\n",
       " 'world',\n",
       " 'wide',\n",
       " 'distribution',\n",
       " 'but',\n",
       " 'the',\n",
       " 'Atlantic',\n",
       " 'and',\n",
       " 'Pacific',\n",
       " 'populations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'northern',\n",
       " 'hemisphere',\n",
       " 'appear',\n",
       " 'to',\n",
       " 'be',\n",
       " 'discrete',\n",
       " 'populations',\n",
       " 'as',\n",
       " 'is',\n",
       " 'the',\n",
       " 'population',\n",
       " 'of',\n",
       " 'the',\n",
       " 'southern',\n",
       " 'hemispheric',\n",
       " 'oceans',\n",
       " 'Each',\n",
       " 'of',\n",
       " 'these',\n",
       " 'oceanic',\n",
       " 'populations',\n",
       " 'may',\n",
       " 'be',\n",
       " 'further',\n",
       " 'subdivided',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'isolates',\n",
       " 'each',\n",
       " 'with',\n",
       " 'its',\n",
       " 'own',\n",
       " 'migratory',\n",
       " 'pattern',\n",
       " 'and',\n",
       " 'somewhat',\n",
       " 'distinct',\n",
       " 'gene',\n",
       " 'pool',\n",
       " 'This',\n",
       " 'study',\n",
       " 'will',\n",
       " 'provide',\n",
       " 'information',\n",
       " 'on',\n",
       " 'the',\n",
       " 'level',\n",
       " 'of',\n",
       " 'genetic',\n",
       " 'isolation',\n",
       " 'among',\n",
       " 'populations',\n",
       " 'and',\n",
       " 'the',\n",
       " 'levels',\n",
       " 'of',\n",
       " 'gene',\n",
       " 'flow',\n",
       " 'and',\n",
       " 'genealogical',\n",
       " 'relationships',\n",
       " 'among',\n",
       " 'populations',\n",
       " 'This',\n",
       " 'detailed',\n",
       " 'genetic',\n",
       " 'information',\n",
       " 'will',\n",
       " 'facilitate',\n",
       " 'international',\n",
       " 'policy',\n",
       " 'decisions',\n",
       " 'regarding',\n",
       " 'the',\n",
       " 'conservation',\n",
       " 'and',\n",
       " 'management',\n",
       " 'of',\n",
       " 'these',\n",
       " 'magnificent',\n",
       " 'mammals']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0052\tresearch\n",
      "0.0052\tAward\n",
      "0.0040\tproject\n",
      "0.0040\thave\n",
      "0.0035\tNSF\n",
      "0.0034\tInvestigator\n",
      "0.0034\tEstimated\n",
      "0.0034\tProgram\n",
      "0.0033\tdata\n",
      "0.0032\tmodels\n",
      "\n",
      "Topic 1\n",
      "0.0221\tAward\n",
      "0.0163\tInvestigator\n",
      "0.0150\tProgram\n",
      "0.0148\tNSF\n",
      "0.0146\tEstimated\n",
      "0.0096\tMATHEMATICAL\n",
      "0.0090\tPrincipal\n",
      "0.0087\tcurrent\n",
      "0.0080\tAbstract\n",
      "0.0079\tDMS\n",
      "\n",
      "Topic 2\n",
      "0.0118\tAward\n",
      "0.0100\tInvestigator\n",
      "0.0084\tProgram\n",
      "0.0082\tNSF\n",
      "0.0079\tEstimated\n",
      "0.0062\tPrincipal\n",
      "0.0062\tcurrent\n",
      "0.0043\tSCIENCES\n",
      "0.0042\tAbstract\n",
      "0.0041\tGEO\n",
      "\n",
      "Topic 3\n",
      "0.0613\tNC\n",
      "0.0361\t919\n",
      "0.0331\tHill\n",
      "0.0281\tCarolina\n",
      "0.0265\tChapel\n",
      "0.0170\tNorth\n",
      "0.0114\t27514\n",
      "0.0106\tunc\n",
      "0.0090\t962\n",
      "0.0074\tBynum\n",
      "\n",
      "Topic 4\n",
      "0.0129\tInvestigator\n",
      "0.0110\tAward\n",
      "0.0102\tstudents\n",
      "0.0092\tPrincipal\n",
      "0.0092\tcurrent\n",
      "0.0087\tNSF\n",
      "0.0084\tProgram\n",
      "0.0081\tresearch\n",
      "0.0073\tEstimated\n",
      "0.0054\tUniversity\n",
      "\n",
      "Topic 5\n",
      "0.0246\tRutgers\n",
      "0.0234\tNew\n",
      "0.0191\tBrunswick\n",
      "0.0169\tAmherst\n",
      "0.0155\tYale\n",
      "0.0148\tPlaza\n",
      "0.0141\t408\n",
      "0.0127\tIII\n",
      "0.0124\tCruz\n",
      "0.0121\tSanta\n",
      "\n",
      "Topic 6\n",
      "0.0098\tAward\n",
      "0.0076\tInvestigator\n",
      "0.0072\tProgram\n",
      "0.0066\tNSF\n",
      "0.0065\tEstimated\n",
      "0.0047\tcurrent\n",
      "0.0044\tPrincipal\n",
      "0.0043\tresearch\n",
      "0.0034\tmaterials\n",
      "0.0034\tAbstract\n",
      "\n",
      "Topic 7\n",
      "0.0115\tAward\n",
      "0.0101\tInvestigator\n",
      "0.0078\tNSF\n",
      "0.0076\tEstimated\n",
      "0.0074\tProgram\n",
      "0.0066\tcurrent\n",
      "0.0064\tdesign\n",
      "0.0064\tsystems\n",
      "0.0062\tPrincipal\n",
      "0.0058\tsystem\n",
      "\n",
      "Topic 8\n",
      "0.0320\tspeech\n",
      "0.0188\tChampaign\n",
      "0.0186\tNMR\n",
      "0.0173\tlanguage\n",
      "0.0158\tSanta\n",
      "0.0142\tPittsburgh\n",
      "0.0142\tBarbara\n",
      "0.0107\tUrbana\n",
      "0.0106\tWright\n",
      "0.0104\tIrvine\n",
      "\n",
      "Topic 9\n",
      "0.0082\tAward\n",
      "0.0063\tInvestigator\n",
      "0.0055\tNSF\n",
      "0.0054\tEstimated\n",
      "0.0054\tProgram\n",
      "0.0039\tBIOLOGICAL\n",
      "0.0036\tcurrent\n",
      "0.0036\tPrincipal\n",
      "0.0032\tcell\n",
      "0.0031\tBIO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"the of and to for in or The is be may an a with has at these This Date this This OF that are will which on by this as from can\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### above the topics are much more meainingful than the first one but with removing some words which are repeated in all the topics, th extracted topics might make more sense, so some words such as 'program Program Award award Inverstigator inverstigator Estimated estimated University university Principal' will be excluded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0113\tbrain\n",
      "0.0108\tmarket\n",
      "0.0104\tPittsburgh\n",
      "0.0099\tfirms\n",
      "0.0082\tfinancial\n",
      "0.0080\tECONOMIC\n",
      "0.0078\tCarnegie\n",
      "0.0076\tSOCIAL\n",
      "0.0075\tINT\n",
      "0.0071\tMellon\n",
      "\n",
      "Topic 1\n",
      "0.0142\tCOMPUTER\n",
      "0.0107\tHPCC\n",
      "0.0100\tINFO\n",
      "0.0099\tCSE\n",
      "0.0099\tENGINR\n",
      "0.0094\tSCIE\n",
      "0.0089\tsystems\n",
      "0.0083\tRESEARCH\n",
      "0.0083\tdata\n",
      "0.0082\tdesign\n",
      "\n",
      "Topic 2\n",
      "0.0046\tNSF\n",
      "0.0046\ttheory\n",
      "0.0040\tproblems\n",
      "0.0038\tsuch\n",
      "0.0037\tDMS\n",
      "0.0035\tmodels\n",
      "0.0033\tsystems\n",
      "0.0032\thave\n",
      "0.0031\tMATHEMATICAL\n",
      "0.0029\tmethods\n",
      "\n",
      "Topic 3\n",
      "0.0049\tspecies\n",
      "0.0047\tBIOLOGICAL\n",
      "0.0046\tNSF\n",
      "0.0036\tBIO\n",
      "0.0034\thave\n",
      "0.0031\tSCIENCES\n",
      "0.0030\tcurrent\n",
      "0.0028\tBIOLOGY\n",
      "0.0027\tprotein\n",
      "0.0027\tcell\n",
      "\n",
      "Topic 4\n",
      "0.0118\tstudents\n",
      "0.0069\tcurrent\n",
      "0.0067\t2003\n",
      "0.0062\t2002\n",
      "0.0061\tNSF\n",
      "0.0059\tscience\n",
      "0.0037\tgraduate\n",
      "0.0036\tundergraduate\n",
      "0.0035\teducation\n",
      "0.0033\tengineering\n",
      "\n",
      "Topic 5\n",
      "0.0071\tNSF\n",
      "0.0070\t2002\n",
      "0.0055\tcurrent\n",
      "0.0055\tDIRECTORATE\n",
      "0.0052\tENGINEERING\n",
      "0.0045\tENG\n",
      "0.0033\tNumber\n",
      "0.0033\tManager\n",
      "0.0032\tType\n",
      "0.0032\tExpected\n",
      "\n",
      "Topic 6\n",
      "0.0076\t2002\n",
      "0.0073\tNSF\n",
      "0.0067\t2003\n",
      "0.0059\tGEO\n",
      "0.0058\tcurrent\n",
      "0.0058\tclimate\n",
      "0.0057\tGEOSCIENCES\n",
      "0.0057\tSCIENCES\n",
      "0.0048\tEAR\n",
      "0.0047\tDIVISION\n",
      "\n",
      "Topic 7\n",
      "0.0052\tNSF\n",
      "0.0046\tdata\n",
      "0.0036\tSOCIAL\n",
      "0.0034\tECONOMIC\n",
      "0.0033\thave\n",
      "0.0031\tcurrent\n",
      "0.0029\tstudy\n",
      "0.0026\thow\n",
      "0.0026\tGrant\n",
      "0.0025\tStandard\n",
      "\n",
      "Topic 8\n",
      "0.0063\tmaterials\n",
      "0.0052\tNSF\n",
      "0.0035\tcurrent\n",
      "0.0034\thigh\n",
      "0.0033\tproperties\n",
      "0.0031\tnew\n",
      "0.0029\tPHYSICAL\n",
      "0.0029\tMPS\n",
      "0.0027\tMATHEMATICAL\n",
      "0.0027\tSCIEN\n",
      "\n",
      "Topic 9\n",
      "0.0199\tNSF\n",
      "0.0168\tcurrent\n",
      "0.0141\t2003\n",
      "0.0105\t2001\n",
      "0.0102\t2002\n",
      "0.0096\tNumber\n",
      "0.0095\tRef\n",
      "0.0095\tOrg\n",
      "0.0095\tFld\n",
      "0.0095\tType\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"the of and to for in or The project is be may an a with has at these This Date this This OF that are will which on by this as from can  program Program Award award research III FOR Co Prgm Abstract Investigator their it inverstigator Estimated estimated University university Principal \".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0 : market and finance\n",
    "Topic 1: computer science\n",
    "toipc 2: mathematical methods \n",
    "Topic 3 :Biology\n",
    "Topic 4: students\n",
    "Topic 5 : -----\n",
    "Topic 6 : climate and GEO science\n",
    "Topic 7 : social and econimic \n",
    "Topic 8 : material property\n",
    "Topic 9: ------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 14:59:10,764 : INFO : collecting all words and their counts\n",
      "2020-03-17 14:59:10,769 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-17 14:59:11,908 : INFO : PROGRESS: at sentence #10000, processed 2451772 words, keeping 92911 word types\n",
      "2020-03-17 14:59:14,371 : INFO : PROGRESS: at sentence #20000, processed 4887339 words, keeping 147492 word types\n",
      "2020-03-17 14:59:17,341 : INFO : PROGRESS: at sentence #30000, processed 7444097 words, keeping 195641 word types\n",
      "2020-03-17 14:59:20,219 : INFO : PROGRESS: at sentence #40000, processed 10105855 words, keeping 241403 word types\n",
      "2020-03-17 14:59:23,663 : INFO : PROGRESS: at sentence #50000, processed 12897489 words, keeping 285562 word types\n",
      "2020-03-17 14:59:27,533 : INFO : PROGRESS: at sentence #60000, processed 15856587 words, keeping 328878 word types\n",
      "2020-03-17 14:59:31,725 : INFO : PROGRESS: at sentence #70000, processed 18894877 words, keeping 371074 word types\n",
      "2020-03-17 14:59:36,078 : INFO : PROGRESS: at sentence #80000, processed 21977516 words, keeping 412277 word types\n",
      "2020-03-17 14:59:40,427 : INFO : PROGRESS: at sentence #90000, processed 25127090 words, keeping 453511 word types\n",
      "2020-03-17 14:59:45,270 : INFO : PROGRESS: at sentence #100000, processed 28466642 words, keeping 494018 word types\n",
      "2020-03-17 14:59:49,665 : INFO : PROGRESS: at sentence #110000, processed 31864922 words, keeping 533052 word types\n",
      "2020-03-17 14:59:54,156 : INFO : PROGRESS: at sentence #120000, processed 35372607 words, keeping 572874 word types\n",
      "2020-03-17 14:59:58,719 : INFO : PROGRESS: at sentence #130000, processed 38947643 words, keeping 611623 word types\n",
      "2020-03-17 14:59:59,579 : INFO : collected 618533 word types from a corpus of 39673924 raw words and 132041 sentences\n",
      "2020-03-17 14:59:59,582 : INFO : Loading a fresh vocabulary\n",
      "2020-03-17 15:02:47,175 : INFO : effective_min_count=5 retains 96736 unique words (15% of original 618533, drops 521797)\n",
      "2020-03-17 15:02:47,183 : INFO : effective_min_count=5 leaves 38956037 word corpus (98% of original 39673924, drops 717887)\n",
      "2020-03-17 15:02:47,649 : INFO : deleting the raw counts dictionary of 618533 items\n",
      "2020-03-17 15:02:48,417 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2020-03-17 15:02:48,418 : INFO : downsampling leaves estimated 31932245 word corpus (82.0% of prior 38956037)\n",
      "2020-03-17 15:02:48,835 : INFO : estimated required memory for 96736 words and 100 dimensions: 125756800 bytes\n",
      "2020-03-17 15:02:48,836 : INFO : resetting layer weights\n",
      "2020-03-17 15:03:04,427 : INFO : training model with 4 workers on 96736 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-17 15:03:05,470 : INFO : EPOCH 1 - PROGRESS: at 1.93% examples, 502036 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:06,486 : INFO : EPOCH 1 - PROGRESS: at 4.94% examples, 647044 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:07,496 : INFO : EPOCH 1 - PROGRESS: at 8.01% examples, 683075 words/s, in_qsize 8, out_qsize 3\n",
      "2020-03-17 15:03:08,510 : INFO : EPOCH 1 - PROGRESS: at 11.19% examples, 719626 words/s, in_qsize 5, out_qsize 2\n",
      "2020-03-17 15:03:09,514 : INFO : EPOCH 1 - PROGRESS: at 14.16% examples, 733408 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-17 15:03:10,531 : INFO : EPOCH 1 - PROGRESS: at 17.18% examples, 731651 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:03:11,536 : INFO : EPOCH 1 - PROGRESS: at 19.81% examples, 730815 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:03:12,537 : INFO : EPOCH 1 - PROGRESS: at 22.44% examples, 730720 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:13,538 : INFO : EPOCH 1 - PROGRESS: at 25.29% examples, 730201 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:14,558 : INFO : EPOCH 1 - PROGRESS: at 27.84% examples, 731751 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:03:15,559 : INFO : EPOCH 1 - PROGRESS: at 30.36% examples, 733550 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-17 15:03:16,568 : INFO : EPOCH 1 - PROGRESS: at 33.07% examples, 734588 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:03:17,572 : INFO : EPOCH 1 - PROGRESS: at 35.65% examples, 737518 words/s, in_qsize 7, out_qsize 2\n",
      "2020-03-17 15:03:18,584 : INFO : EPOCH 1 - PROGRESS: at 38.07% examples, 737899 words/s, in_qsize 8, out_qsize 2\n",
      "2020-03-17 15:03:19,592 : INFO : EPOCH 1 - PROGRESS: at 40.38% examples, 735455 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:03:20,593 : INFO : EPOCH 1 - PROGRESS: at 42.35% examples, 732839 words/s, in_qsize 5, out_qsize 2\n",
      "2020-03-17 15:03:21,616 : INFO : EPOCH 1 - PROGRESS: at 44.67% examples, 731435 words/s, in_qsize 8, out_qsize 2\n",
      "2020-03-17 15:03:22,641 : INFO : EPOCH 1 - PROGRESS: at 46.97% examples, 727425 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:03:23,643 : INFO : EPOCH 1 - PROGRESS: at 48.87% examples, 721852 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:24,645 : INFO : EPOCH 1 - PROGRESS: at 50.97% examples, 719779 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:25,659 : INFO : EPOCH 1 - PROGRESS: at 53.22% examples, 720391 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-17 15:03:26,689 : INFO : EPOCH 1 - PROGRESS: at 55.40% examples, 721177 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-17 15:03:27,692 : INFO : EPOCH 1 - PROGRESS: at 57.75% examples, 720660 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:03:28,696 : INFO : EPOCH 1 - PROGRESS: at 60.05% examples, 722020 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:29,715 : INFO : EPOCH 1 - PROGRESS: at 62.14% examples, 721097 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:03:30,720 : INFO : EPOCH 1 - PROGRESS: at 64.05% examples, 718451 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:31,736 : INFO : EPOCH 1 - PROGRESS: at 66.28% examples, 717153 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:32,739 : INFO : EPOCH 1 - PROGRESS: at 68.33% examples, 717205 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:03:33,753 : INFO : EPOCH 1 - PROGRESS: at 70.55% examples, 717990 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:34,780 : INFO : EPOCH 1 - PROGRESS: at 72.58% examples, 715846 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-17 15:03:35,793 : INFO : EPOCH 1 - PROGRESS: at 74.45% examples, 715354 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:36,803 : INFO : EPOCH 1 - PROGRESS: at 76.22% examples, 713323 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:37,819 : INFO : EPOCH 1 - PROGRESS: at 78.39% examples, 713404 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:38,856 : INFO : EPOCH 1 - PROGRESS: at 80.21% examples, 711586 words/s, in_qsize 8, out_qsize 3\n",
      "2020-03-17 15:03:39,862 : INFO : EPOCH 1 - PROGRESS: at 82.19% examples, 712350 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:03:40,884 : INFO : EPOCH 1 - PROGRESS: at 84.13% examples, 711868 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:03:41,895 : INFO : EPOCH 1 - PROGRESS: at 85.99% examples, 710933 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:42,909 : INFO : EPOCH 1 - PROGRESS: at 87.93% examples, 711614 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:43,917 : INFO : EPOCH 1 - PROGRESS: at 89.94% examples, 711893 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:03:44,938 : INFO : EPOCH 1 - PROGRESS: at 91.87% examples, 711663 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:03:45,953 : INFO : EPOCH 1 - PROGRESS: at 93.72% examples, 711164 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:46,966 : INFO : EPOCH 1 - PROGRESS: at 95.30% examples, 708906 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:47,999 : INFO : EPOCH 1 - PROGRESS: at 97.14% examples, 708533 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:49,017 : INFO : EPOCH 1 - PROGRESS: at 99.07% examples, 709000 words/s, in_qsize 8, out_qsize 3\n",
      "2020-03-17 15:03:49,404 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:03:49,406 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:03:49,410 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:03:49,418 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:03:49,418 : INFO : EPOCH - 1 : training on 39673924 raw words (31933825 effective words) took 45.0s, 710093 effective words/s\n",
      "2020-03-17 15:03:50,427 : INFO : EPOCH 2 - PROGRESS: at 5.57% examples, 1488056 words/s, in_qsize 8, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 15:03:51,430 : INFO : EPOCH 2 - PROGRESS: at 11.31% examples, 1473681 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:52,432 : INFO : EPOCH 2 - PROGRESS: at 16.57% examples, 1426702 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:03:53,439 : INFO : EPOCH 2 - PROGRESS: at 21.64% examples, 1420707 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:54,445 : INFO : EPOCH 2 - PROGRESS: at 26.10% examples, 1372532 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:55,450 : INFO : EPOCH 2 - PROGRESS: at 30.30% examples, 1349919 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:03:56,469 : INFO : EPOCH 2 - PROGRESS: at 34.72% examples, 1333724 words/s, in_qsize 5, out_qsize 2\n",
      "2020-03-17 15:03:57,478 : INFO : EPOCH 2 - PROGRESS: at 38.84% examples, 1323950 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:58,482 : INFO : EPOCH 2 - PROGRESS: at 42.53% examples, 1313139 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:03:59,491 : INFO : EPOCH 2 - PROGRESS: at 46.83% examples, 1309242 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-17 15:04:00,493 : INFO : EPOCH 2 - PROGRESS: at 51.87% examples, 1340412 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:01,502 : INFO : EPOCH 2 - PROGRESS: at 56.14% examples, 1345032 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:02,508 : INFO : EPOCH 2 - PROGRESS: at 60.00% examples, 1336953 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:03,509 : INFO : EPOCH 2 - PROGRESS: at 63.49% examples, 1326215 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:04,509 : INFO : EPOCH 2 - PROGRESS: at 67.27% examples, 1318868 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:05,510 : INFO : EPOCH 2 - PROGRESS: at 70.05% examples, 1297381 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:06,511 : INFO : EPOCH 2 - PROGRESS: at 73.91% examples, 1300384 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:07,514 : INFO : EPOCH 2 - PROGRESS: at 78.23% examples, 1312630 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:08,518 : INFO : EPOCH 2 - PROGRESS: at 82.67% examples, 1330708 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:09,538 : INFO : EPOCH 2 - PROGRESS: at 87.02% examples, 1343268 words/s, in_qsize 8, out_qsize 2\n",
      "2020-03-17 15:04:10,538 : INFO : EPOCH 2 - PROGRESS: at 90.46% examples, 1340456 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:11,544 : INFO : EPOCH 2 - PROGRESS: at 94.78% examples, 1353326 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:04:12,553 : INFO : EPOCH 2 - PROGRESS: at 98.92% examples, 1363820 words/s, in_qsize 5, out_qsize 3\n",
      "2020-03-17 15:04:12,742 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:04:12,745 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:04:12,746 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:04:12,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:04:12,750 : INFO : EPOCH - 2 : training on 39673924 raw words (31931899 effective words) took 23.3s, 1368915 effective words/s\n",
      "2020-03-17 15:04:13,759 : INFO : EPOCH 3 - PROGRESS: at 4.74% examples, 1256863 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:14,771 : INFO : EPOCH 3 - PROGRESS: at 10.05% examples, 1295546 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:15,782 : INFO : EPOCH 3 - PROGRESS: at 15.18% examples, 1299294 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:16,784 : INFO : EPOCH 3 - PROGRESS: at 20.15% examples, 1309279 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:17,786 : INFO : EPOCH 3 - PROGRESS: at 25.17% examples, 1311416 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:18,786 : INFO : EPOCH 3 - PROGRESS: at 31.60% examples, 1402762 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:19,798 : INFO : EPOCH 3 - PROGRESS: at 36.21% examples, 1400782 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:20,801 : INFO : EPOCH 3 - PROGRESS: at 40.57% examples, 1392847 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:21,803 : INFO : EPOCH 3 - PROGRESS: at 44.49% examples, 1381549 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:04:22,804 : INFO : EPOCH 3 - PROGRESS: at 48.74% examples, 1375646 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:23,808 : INFO : EPOCH 3 - PROGRESS: at 52.64% examples, 1365103 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:24,809 : INFO : EPOCH 3 - PROGRESS: at 56.78% examples, 1363423 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:25,818 : INFO : EPOCH 3 - PROGRESS: at 60.88% examples, 1361493 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:26,823 : INFO : EPOCH 3 - PROGRESS: at 65.00% examples, 1360781 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:27,831 : INFO : EPOCH 3 - PROGRESS: at 68.85% examples, 1357366 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:28,841 : INFO : EPOCH 3 - PROGRESS: at 72.87% examples, 1356152 words/s, in_qsize 5, out_qsize 2\n",
      "2020-03-17 15:04:29,842 : INFO : EPOCH 3 - PROGRESS: at 78.09% examples, 1386802 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:30,843 : INFO : EPOCH 3 - PROGRESS: at 83.11% examples, 1414190 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:31,858 : INFO : EPOCH 3 - PROGRESS: at 87.50% examples, 1424447 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:32,861 : INFO : EPOCH 3 - PROGRESS: at 90.99% examples, 1417810 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:33,863 : INFO : EPOCH 3 - PROGRESS: at 94.45% examples, 1412203 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:34,867 : INFO : EPOCH 3 - PROGRESS: at 97.74% examples, 1405862 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:04:35,553 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:04:35,555 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:04:35,560 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:04:35,570 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:04:35,570 : INFO : EPOCH - 3 : training on 39673924 raw words (31933983 effective words) took 22.8s, 1399566 effective words/s\n",
      "2020-03-17 15:04:36,578 : INFO : EPOCH 4 - PROGRESS: at 5.17% examples, 1380235 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:37,582 : INFO : EPOCH 4 - PROGRESS: at 11.67% examples, 1520968 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:38,585 : INFO : EPOCH 4 - PROGRESS: at 16.86% examples, 1449989 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:39,590 : INFO : EPOCH 4 - PROGRESS: at 22.86% examples, 1498776 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:40,599 : INFO : EPOCH 4 - PROGRESS: at 29.09% examples, 1553159 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:41,613 : INFO : EPOCH 4 - PROGRESS: at 33.84% examples, 1514305 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:42,614 : INFO : EPOCH 4 - PROGRESS: at 38.18% examples, 1485606 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:04:43,614 : INFO : EPOCH 4 - PROGRESS: at 42.20% examples, 1465136 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:44,616 : INFO : EPOCH 4 - PROGRESS: at 46.73% examples, 1453644 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:45,625 : INFO : EPOCH 4 - PROGRESS: at 50.82% examples, 1441209 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:46,631 : INFO : EPOCH 4 - PROGRESS: at 54.82% examples, 1433131 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:04:47,632 : INFO : EPOCH 4 - PROGRESS: at 59.04% examples, 1425937 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:48,649 : INFO : EPOCH 4 - PROGRESS: at 63.15% examples, 1419149 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:49,653 : INFO : EPOCH 4 - PROGRESS: at 68.60% examples, 1447549 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:50,656 : INFO : EPOCH 4 - PROGRESS: at 74.07% examples, 1477050 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:51,658 : INFO : EPOCH 4 - PROGRESS: at 79.27% examples, 1500390 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:52,662 : INFO : EPOCH 4 - PROGRESS: at 83.34% examples, 1501813 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:53,665 : INFO : EPOCH 4 - PROGRESS: at 86.95% examples, 1491822 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:54,681 : INFO : EPOCH 4 - PROGRESS: at 90.56% examples, 1483502 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:55,696 : INFO : EPOCH 4 - PROGRESS: at 94.11% examples, 1474127 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 15:04:56,711 : INFO : EPOCH 4 - PROGRESS: at 97.43% examples, 1465623 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:57,432 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:04:57,433 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:04:57,436 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:04:57,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:04:57,438 : INFO : EPOCH - 4 : training on 39673924 raw words (31931616 effective words) took 21.9s, 1460647 effective words/s\n",
      "2020-03-17 15:04:58,459 : INFO : EPOCH 5 - PROGRESS: at 5.00% examples, 1316750 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:04:59,461 : INFO : EPOCH 5 - PROGRESS: at 10.25% examples, 1323970 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:00,463 : INFO : EPOCH 5 - PROGRESS: at 15.50% examples, 1327349 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:01,467 : INFO : EPOCH 5 - PROGRESS: at 20.36% examples, 1325313 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:02,468 : INFO : EPOCH 5 - PROGRESS: at 25.39% examples, 1327825 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:03,471 : INFO : EPOCH 5 - PROGRESS: at 29.63% examples, 1319341 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:05:04,475 : INFO : EPOCH 5 - PROGRESS: at 34.30% examples, 1319428 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:05,480 : INFO : EPOCH 5 - PROGRESS: at 38.46% examples, 1312804 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:06,482 : INFO : EPOCH 5 - PROGRESS: at 42.44% examples, 1312554 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:07,485 : INFO : EPOCH 5 - PROGRESS: at 46.95% examples, 1317206 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:08,491 : INFO : EPOCH 5 - PROGRESS: at 51.23% examples, 1323100 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:05:09,491 : INFO : EPOCH 5 - PROGRESS: at 55.27% examples, 1327487 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:10,492 : INFO : EPOCH 5 - PROGRESS: at 60.00% examples, 1340511 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:05:11,495 : INFO : EPOCH 5 - PROGRESS: at 63.95% examples, 1341163 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-17 15:05:12,495 : INFO : EPOCH 5 - PROGRESS: at 68.07% examples, 1341249 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:13,501 : INFO : EPOCH 5 - PROGRESS: at 71.93% examples, 1337884 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:14,503 : INFO : EPOCH 5 - PROGRESS: at 75.34% examples, 1334405 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:15,508 : INFO : EPOCH 5 - PROGRESS: at 79.15% examples, 1333263 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:16,517 : INFO : EPOCH 5 - PROGRESS: at 82.65% examples, 1331717 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:17,521 : INFO : EPOCH 5 - PROGRESS: at 86.34% examples, 1332336 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:18,524 : INFO : EPOCH 5 - PROGRESS: at 89.96% examples, 1333281 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:05:19,527 : INFO : EPOCH 5 - PROGRESS: at 93.53% examples, 1333132 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:20,532 : INFO : EPOCH 5 - PROGRESS: at 96.90% examples, 1332778 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:05:21,378 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:05:21,382 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:05:21,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:05:21,386 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:05:21,386 : INFO : EPOCH - 5 : training on 39673924 raw words (31930397 effective words) took 23.9s, 1333721 effective words/s\n",
      "2020-03-17 15:05:21,387 : INFO : training on a 198369620 raw words (159661720 effective words) took 137.0s, 1165767 effective words/s\n"
     ]
    }
   ],
   "source": [
    "## Train word vectors\n",
    "# Make sure you also have cython installed to accelerate computation!\n",
    "import gensim \n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Train word2vec model\n",
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['work', 'pattern', 'disease', 'percent', 'obvious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('research', 0.8633078336715698), ('study', 0.7425322532653809), ('project', 0.7225260734558105), ('effort', 0.6897100210189819), ('investigation', 0.6768474578857422), ('studies', 0.6582659482955933), ('investigations', 0.6531093716621399), ('proposal', 0.6226323246955872), ('workshop', 0.5873894691467285), ('experiment', 0.5842708945274353)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('work'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the selected words by the model except 'workshop' can be accepted to have same meaning for differnt situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('patterns', 0.789414644241333), ('timing', 0.6502902507781982), ('mode', 0.6290395259857178), ('temporal', 0.6176190376281738), ('asymmetries', 0.6067225337028503), ('character', 0.5989295244216919), ('spontaneous', 0.5971380472183228), ('plasticity', 0.5966061353683472), ('appearance', 0.5956093072891235), ('shape', 0.5931124091148376)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('pattern'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the selected words by the model are in good match with the 'pattern' anyhow 'spontanious' and 'plasticity' look to have differnt meaning but can be company the word 'pattern' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('diseases', 0.8456560373306274), ('AIDS', 0.7965543270111084), ('pathogens', 0.7953171730041504), ('infectious', 0.7715359926223755), ('cancer', 0.7692543268203735), ('infections', 0.765235424041748), ('pathogen', 0.7580274343490601), ('infection', 0.7465482354164124), ('pests', 0.7456491589546204), ('HIV', 0.7343069314956665)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('disease'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for word of 'disease', mostly name of the disease and words which are companying the disease have been recognized in good percision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('approximately', 0.7446668148040771), ('total', 0.7442915439605713), ('75', 0.7309954762458801), ('80', 0.7300984263420105), ('billion', 0.728751540184021), ('70', 0.7251726388931274), ('million', 0.7248543500900269), ('hectares', 0.7217522859573364), ('upwards', 0.7165240049362183), ('tons', 0.7069244384765625)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('percent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For percent the selected words look to be less related, words such as 'tones' , 'hectares' and so on do not look to be a good match for percent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('serious', 0.7301241159439087), ('apparent', 0.7041081190109253), ('surprising', 0.6860836148262024), ('questionable', 0.6812020540237427), ('inevitable', 0.6781014204025269), ('notable', 0.6759514808654785), ('however', 0.6751658320426941), ('compelling', 0.6696269512176514), ('problematic', 0.6669461727142334), ('immense', 0.6619775295257568)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('obvious'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### not very good for 'obvious' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change some parameters in the model to improve the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 15:28:07,015 : INFO : collecting all words and their counts\n",
      "2020-03-17 15:28:07,016 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-17 15:28:07,444 : INFO : PROGRESS: at sentence #10000, processed 2451772 words, keeping 92911 word types\n",
      "2020-03-17 15:28:07,844 : INFO : PROGRESS: at sentence #20000, processed 4887339 words, keeping 147492 word types\n",
      "2020-03-17 15:28:08,289 : INFO : PROGRESS: at sentence #30000, processed 7444097 words, keeping 195641 word types\n",
      "2020-03-17 15:28:08,735 : INFO : PROGRESS: at sentence #40000, processed 10105855 words, keeping 241403 word types\n",
      "2020-03-17 15:28:09,210 : INFO : PROGRESS: at sentence #50000, processed 12897489 words, keeping 285562 word types\n",
      "2020-03-17 15:28:09,711 : INFO : PROGRESS: at sentence #60000, processed 15856587 words, keeping 328878 word types\n",
      "2020-03-17 15:28:10,246 : INFO : PROGRESS: at sentence #70000, processed 18894877 words, keeping 371074 word types\n",
      "2020-03-17 15:28:10,788 : INFO : PROGRESS: at sentence #80000, processed 21977516 words, keeping 412277 word types\n",
      "2020-03-17 15:28:11,324 : INFO : PROGRESS: at sentence #90000, processed 25127090 words, keeping 453511 word types\n",
      "2020-03-17 15:28:11,909 : INFO : PROGRESS: at sentence #100000, processed 28466642 words, keeping 494018 word types\n",
      "2020-03-17 15:28:12,501 : INFO : PROGRESS: at sentence #110000, processed 31864922 words, keeping 533052 word types\n",
      "2020-03-17 15:28:13,075 : INFO : PROGRESS: at sentence #120000, processed 35372607 words, keeping 572874 word types\n",
      "2020-03-17 15:28:13,703 : INFO : PROGRESS: at sentence #130000, processed 38947643 words, keeping 611623 word types\n",
      "2020-03-17 15:28:13,834 : INFO : collected 618533 word types from a corpus of 39673924 raw words and 132041 sentences\n",
      "2020-03-17 15:28:13,835 : INFO : Loading a fresh vocabulary\n",
      "2020-03-17 15:28:14,194 : INFO : effective_min_count=10 retains 61051 unique words (9% of original 618533, drops 557482)\n",
      "2020-03-17 15:28:14,195 : INFO : effective_min_count=10 leaves 38723429 word corpus (97% of original 39673924, drops 950495)\n",
      "2020-03-17 15:28:14,374 : INFO : deleting the raw counts dictionary of 618533 items\n",
      "2020-03-17 15:28:14,391 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2020-03-17 15:28:14,391 : INFO : downsampling leaves estimated 31672940 word corpus (81.8% of prior 38723429)\n",
      "2020-03-17 15:28:14,574 : INFO : estimated required memory for 61051 words and 200 dimensions: 128207100 bytes\n",
      "2020-03-17 15:28:14,575 : INFO : resetting layer weights\n",
      "2020-03-17 15:28:23,923 : INFO : training model with 4 workers on 61051 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-17 15:28:24,942 : INFO : EPOCH 1 - PROGRESS: at 3.85% examples, 1017127 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:25,952 : INFO : EPOCH 1 - PROGRESS: at 8.31% examples, 1062407 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:28:26,962 : INFO : EPOCH 1 - PROGRESS: at 12.46% examples, 1069295 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:27,971 : INFO : EPOCH 1 - PROGRESS: at 16.95% examples, 1080376 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:28,972 : INFO : EPOCH 1 - PROGRESS: at 20.95% examples, 1085965 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:29,977 : INFO : EPOCH 1 - PROGRESS: at 25.26% examples, 1090149 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:30,979 : INFO : EPOCH 1 - PROGRESS: at 28.89% examples, 1090838 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:31,982 : INFO : EPOCH 1 - PROGRESS: at 32.88% examples, 1090908 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:32,983 : INFO : EPOCH 1 - PROGRESS: at 37.02% examples, 1111348 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:33,997 : INFO : EPOCH 1 - PROGRESS: at 40.69% examples, 1109643 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:35,002 : INFO : EPOCH 1 - PROGRESS: at 44.53% examples, 1122760 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:36,002 : INFO : EPOCH 1 - PROGRESS: at 48.67% examples, 1135117 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:37,019 : INFO : EPOCH 1 - PROGRESS: at 52.01% examples, 1129589 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:38,025 : INFO : EPOCH 1 - PROGRESS: at 55.25% examples, 1125841 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:39,028 : INFO : EPOCH 1 - PROGRESS: at 58.79% examples, 1124127 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:40,039 : INFO : EPOCH 1 - PROGRESS: at 62.12% examples, 1122295 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:41,039 : INFO : EPOCH 1 - PROGRESS: at 65.35% examples, 1117005 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-17 15:28:42,050 : INFO : EPOCH 1 - PROGRESS: at 68.53% examples, 1114928 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:43,067 : INFO : EPOCH 1 - PROGRESS: at 71.72% examples, 1110758 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:44,069 : INFO : EPOCH 1 - PROGRESS: at 74.80% examples, 1111254 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:45,086 : INFO : EPOCH 1 - PROGRESS: at 77.99% examples, 1110069 words/s, in_qsize 7, out_qsize 1\n",
      "2020-03-17 15:28:46,090 : INFO : EPOCH 1 - PROGRESS: at 80.96% examples, 1108794 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:47,100 : INFO : EPOCH 1 - PROGRESS: at 83.84% examples, 1106706 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:48,114 : INFO : EPOCH 1 - PROGRESS: at 86.85% examples, 1105643 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:49,124 : INFO : EPOCH 1 - PROGRESS: at 89.82% examples, 1104766 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:50,132 : INFO : EPOCH 1 - PROGRESS: at 92.74% examples, 1103467 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:51,137 : INFO : EPOCH 1 - PROGRESS: at 95.41% examples, 1101171 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:52,155 : INFO : EPOCH 1 - PROGRESS: at 98.25% examples, 1099433 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:52,728 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:28:52,739 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:28:52,740 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:28:52,745 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:28:52,745 : INFO : EPOCH - 1 : training on 39673924 raw words (31672301 effective words) took 28.8s, 1099499 effective words/s\n",
      "2020-03-17 15:28:53,756 : INFO : EPOCH 2 - PROGRESS: at 3.93% examples, 1041795 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:54,759 : INFO : EPOCH 2 - PROGRESS: at 8.17% examples, 1051026 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:55,769 : INFO : EPOCH 2 - PROGRESS: at 12.40% examples, 1066631 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:56,774 : INFO : EPOCH 2 - PROGRESS: at 16.79% examples, 1073578 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:28:57,785 : INFO : EPOCH 2 - PROGRESS: at 20.76% examples, 1075134 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:28:58,798 : INFO : EPOCH 2 - PROGRESS: at 25.07% examples, 1078466 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:28:59,800 : INFO : EPOCH 2 - PROGRESS: at 29.32% examples, 1107639 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:00,804 : INFO : EPOCH 2 - PROGRESS: at 33.37% examples, 1109110 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:01,813 : INFO : EPOCH 2 - PROGRESS: at 36.95% examples, 1106636 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:02,823 : INFO : EPOCH 2 - PROGRESS: at 40.67% examples, 1107631 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:29:03,827 : INFO : EPOCH 2 - PROGRESS: at 43.90% examples, 1103370 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:04,833 : INFO : EPOCH 2 - PROGRESS: at 47.55% examples, 1103131 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:05,845 : INFO : EPOCH 2 - PROGRESS: at 51.35% examples, 1110657 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:06,855 : INFO : EPOCH 2 - PROGRESS: at 54.55% examples, 1107356 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:29:07,867 : INFO : EPOCH 2 - PROGRESS: at 58.03% examples, 1106190 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:08,870 : INFO : EPOCH 2 - PROGRESS: at 61.41% examples, 1105619 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 15:29:09,887 : INFO : EPOCH 2 - PROGRESS: at 64.73% examples, 1103357 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:10,903 : INFO : EPOCH 2 - PROGRESS: at 67.98% examples, 1101182 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:11,906 : INFO : EPOCH 2 - PROGRESS: at 71.13% examples, 1099507 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:12,910 : INFO : EPOCH 2 - PROGRESS: at 74.16% examples, 1098069 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:13,911 : INFO : EPOCH 2 - PROGRESS: at 77.25% examples, 1098362 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:14,919 : INFO : EPOCH 2 - PROGRESS: at 80.48% examples, 1100263 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:15,919 : INFO : EPOCH 2 - PROGRESS: at 84.48% examples, 1115656 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:16,921 : INFO : EPOCH 2 - PROGRESS: at 88.56% examples, 1132399 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:29:17,925 : INFO : EPOCH 2 - PROGRESS: at 92.21% examples, 1140762 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:18,937 : INFO : EPOCH 2 - PROGRESS: at 95.19% examples, 1139990 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:29:19,947 : INFO : EPOCH 2 - PROGRESS: at 98.17% examples, 1139627 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:20,529 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:29:20,531 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:29:20,542 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:29:20,542 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:29:20,543 : INFO : EPOCH - 2 : training on 39673924 raw words (31671638 effective words) took 27.8s, 1139754 effective words/s\n",
      "2020-03-17 15:29:21,557 : INFO : EPOCH 3 - PROGRESS: at 3.88% examples, 1023796 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:22,557 : INFO : EPOCH 3 - PROGRESS: at 8.17% examples, 1051378 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:23,560 : INFO : EPOCH 3 - PROGRESS: at 12.31% examples, 1062093 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:24,564 : INFO : EPOCH 3 - PROGRESS: at 16.67% examples, 1068075 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:25,567 : INFO : EPOCH 3 - PROGRESS: at 20.60% examples, 1069346 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:26,571 : INFO : EPOCH 3 - PROGRESS: at 24.84% examples, 1072816 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:27,571 : INFO : EPOCH 3 - PROGRESS: at 28.45% examples, 1075073 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:28,579 : INFO : EPOCH 3 - PROGRESS: at 32.49% examples, 1078489 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:29,594 : INFO : EPOCH 3 - PROGRESS: at 36.19% examples, 1081964 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:30,606 : INFO : EPOCH 3 - PROGRESS: at 39.73% examples, 1079040 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:29:31,608 : INFO : EPOCH 3 - PROGRESS: at 43.01% examples, 1080278 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:32,618 : INFO : EPOCH 3 - PROGRESS: at 46.73% examples, 1080621 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:33,619 : INFO : EPOCH 3 - PROGRESS: at 50.14% examples, 1081025 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:34,633 : INFO : EPOCH 3 - PROGRESS: at 53.35% examples, 1080098 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:35,644 : INFO : EPOCH 3 - PROGRESS: at 57.07% examples, 1086746 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:36,649 : INFO : EPOCH 3 - PROGRESS: at 61.08% examples, 1100644 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:37,656 : INFO : EPOCH 3 - PROGRESS: at 65.00% examples, 1110463 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:38,658 : INFO : EPOCH 3 - PROGRESS: at 68.17% examples, 1108287 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:29:39,664 : INFO : EPOCH 3 - PROGRESS: at 71.35% examples, 1105602 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:40,666 : INFO : EPOCH 3 - PROGRESS: at 74.35% examples, 1104058 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:41,681 : INFO : EPOCH 3 - PROGRESS: at 77.42% examples, 1102597 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:42,681 : INFO : EPOCH 3 - PROGRESS: at 80.46% examples, 1101821 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:43,695 : INFO : EPOCH 3 - PROGRESS: at 83.48% examples, 1101925 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:44,699 : INFO : EPOCH 3 - PROGRESS: at 86.58% examples, 1102784 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:45,701 : INFO : EPOCH 3 - PROGRESS: at 89.46% examples, 1101404 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:29:46,712 : INFO : EPOCH 3 - PROGRESS: at 92.42% examples, 1100708 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:29:47,716 : INFO : EPOCH 3 - PROGRESS: at 95.26% examples, 1100047 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:29:48,728 : INFO : EPOCH 3 - PROGRESS: at 98.12% examples, 1099123 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:49,358 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:29:49,362 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:29:49,364 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:29:49,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:29:49,371 : INFO : EPOCH - 3 : training on 39673924 raw words (31673330 effective words) took 28.8s, 1099101 effective words/s\n",
      "2020-03-17 15:29:50,380 : INFO : EPOCH 4 - PROGRESS: at 3.96% examples, 1047848 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:51,384 : INFO : EPOCH 4 - PROGRESS: at 8.27% examples, 1061868 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:52,391 : INFO : EPOCH 4 - PROGRESS: at 12.37% examples, 1064918 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:53,396 : INFO : EPOCH 4 - PROGRESS: at 16.67% examples, 1066293 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:54,399 : INFO : EPOCH 4 - PROGRESS: at 20.60% examples, 1068206 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:55,409 : INFO : EPOCH 4 - PROGRESS: at 24.81% examples, 1069265 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:56,420 : INFO : EPOCH 4 - PROGRESS: at 28.42% examples, 1069307 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:57,421 : INFO : EPOCH 4 - PROGRESS: at 32.41% examples, 1073401 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:29:58,424 : INFO : EPOCH 4 - PROGRESS: at 36.04% examples, 1075700 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:29:59,437 : INFO : EPOCH 4 - PROGRESS: at 39.65% examples, 1076159 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:00,443 : INFO : EPOCH 4 - PROGRESS: at 42.87% examples, 1075879 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:01,450 : INFO : EPOCH 4 - PROGRESS: at 46.37% examples, 1071109 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:02,452 : INFO : EPOCH 4 - PROGRESS: at 49.78% examples, 1072123 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:03,452 : INFO : EPOCH 4 - PROGRESS: at 53.69% examples, 1088537 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:04,459 : INFO : EPOCH 4 - PROGRESS: at 58.38% examples, 1116080 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:05,461 : INFO : EPOCH 4 - PROGRESS: at 61.95% examples, 1119721 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:06,462 : INFO : EPOCH 4 - PROGRESS: at 65.84% examples, 1127802 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:07,472 : INFO : EPOCH 4 - PROGRESS: at 69.02% examples, 1125127 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:08,473 : INFO : EPOCH 4 - PROGRESS: at 72.31% examples, 1123128 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:09,474 : INFO : EPOCH 4 - PROGRESS: at 75.18% examples, 1120625 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:30:10,482 : INFO : EPOCH 4 - PROGRESS: at 78.45% examples, 1120251 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:11,487 : INFO : EPOCH 4 - PROGRESS: at 81.63% examples, 1122319 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:12,490 : INFO : EPOCH 4 - PROGRESS: at 84.61% examples, 1120840 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:13,513 : INFO : EPOCH 4 - PROGRESS: at 87.52% examples, 1118869 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:14,519 : INFO : EPOCH 4 - PROGRESS: at 90.56% examples, 1118293 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 15:30:15,523 : INFO : EPOCH 4 - PROGRESS: at 93.64% examples, 1118730 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:16,523 : INFO : EPOCH 4 - PROGRESS: at 96.30% examples, 1116087 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:17,526 : INFO : EPOCH 4 - PROGRESS: at 99.12% examples, 1114069 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:17,796 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:30:17,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:30:17,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:30:17,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:30:17,812 : INFO : EPOCH - 4 : training on 39673924 raw words (31676254 effective words) took 28.4s, 1113971 effective words/s\n",
      "2020-03-17 15:30:18,827 : INFO : EPOCH 5 - PROGRESS: at 4.10% examples, 1086865 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:19,829 : INFO : EPOCH 5 - PROGRESS: at 9.70% examples, 1247242 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:20,830 : INFO : EPOCH 5 - PROGRESS: at 13.69% examples, 1187689 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:30:21,830 : INFO : EPOCH 5 - PROGRESS: at 18.02% examples, 1161724 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:22,835 : INFO : EPOCH 5 - PROGRESS: at 21.85% examples, 1140775 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:23,845 : INFO : EPOCH 5 - PROGRESS: at 25.97% examples, 1129484 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:24,849 : INFO : EPOCH 5 - PROGRESS: at 29.82% examples, 1131084 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:30:25,863 : INFO : EPOCH 5 - PROGRESS: at 33.70% examples, 1123474 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:26,863 : INFO : EPOCH 5 - PROGRESS: at 37.43% examples, 1124902 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:30:27,876 : INFO : EPOCH 5 - PROGRESS: at 41.14% examples, 1125938 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:28,889 : INFO : EPOCH 5 - PROGRESS: at 44.46% examples, 1120466 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-17 15:30:29,893 : INFO : EPOCH 5 - PROGRESS: at 48.02% examples, 1117763 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:30:30,912 : INFO : EPOCH 5 - PROGRESS: at 51.42% examples, 1112900 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:30:31,924 : INFO : EPOCH 5 - PROGRESS: at 54.59% examples, 1108732 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:32,941 : INFO : EPOCH 5 - PROGRESS: at 58.00% examples, 1105546 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:33,951 : INFO : EPOCH 5 - PROGRESS: at 61.26% examples, 1102050 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:34,965 : INFO : EPOCH 5 - PROGRESS: at 64.58% examples, 1100309 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-17 15:30:35,980 : INFO : EPOCH 5 - PROGRESS: at 67.88% examples, 1098775 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:36,980 : INFO : EPOCH 5 - PROGRESS: at 71.13% examples, 1099410 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:37,981 : INFO : EPOCH 5 - PROGRESS: at 74.60% examples, 1106323 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:38,985 : INFO : EPOCH 5 - PROGRESS: at 77.90% examples, 1107889 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:39,990 : INFO : EPOCH 5 - PROGRESS: at 80.96% examples, 1108098 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:40,993 : INFO : EPOCH 5 - PROGRESS: at 84.01% examples, 1108817 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:42,007 : INFO : EPOCH 5 - PROGRESS: at 87.04% examples, 1108554 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:30:43,017 : INFO : EPOCH 5 - PROGRESS: at 90.09% examples, 1108485 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:44,025 : INFO : EPOCH 5 - PROGRESS: at 92.92% examples, 1105588 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 15:30:45,034 : INFO : EPOCH 5 - PROGRESS: at 95.59% examples, 1103628 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:46,043 : INFO : EPOCH 5 - PROGRESS: at 98.49% examples, 1102427 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 15:30:46,531 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 15:30:46,535 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 15:30:46,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 15:30:46,538 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 15:30:46,539 : INFO : EPOCH - 5 : training on 39673924 raw words (31674821 effective words) took 28.7s, 1103089 effective words/s\n",
      "2020-03-17 15:30:46,539 : INFO : training on a 198369620 raw words (158368344 effective words) took 142.6s, 1110465 effective words/s\n"
     ]
    }
   ],
   "source": [
    "## Train word vectors\n",
    "# Make sure you also have cython installed to accelerate computation!\n",
    "import gensim \n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Train word2vec model\n",
    "vectors = gensim.models.Word2Vec(tokenized_text, size=200, window=5, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 15:42:05,636 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('research', 0.7938613891601562), ('project', 0.7012810707092285), ('study', 0.6698108911514282), ('investigation', 0.6117867231369019), ('effort', 0.6066216230392456), ('investigations', 0.5943707227706909), ('studies', 0.5817960500717163), ('proposal', 0.5413938164710999), ('program', 0.5220478177070618), ('workshop', 0.5200454592704773)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('work'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almost the same selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('patterns', 0.7208275198936462), ('shape', 0.5189458727836609), ('appearance', 0.5178953409194946), ('character', 0.508438229560852), ('temporal', 0.5074548721313477), ('patterning', 0.5058163404464722), ('limb', 0.49612706899642944), ('organ', 0.49492573738098145), ('trajectory', 0.49353599548339844), ('phenomenon', 0.4930301606655121)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('pattern'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### better choices, however, word like patterns does not give any new information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('diseases', 0.8047645092010498), ('AIDS', 0.7582086324691772), ('cancer', 0.7344720959663391), ('infectious', 0.7309648990631104), ('infections', 0.7062660455703735), ('pathogens', 0.7003945112228394), ('HIV', 0.6976037621498108), ('pathogen', 0.6915576457977295), ('Alzheimer', 0.6895266771316528), ('pests', 0.6783614158630371)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('disease'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almost same as the previous model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('approximately', 0.71341872215271), ('75', 0.7007619142532349), ('80', 0.6952838897705078), ('kg', 0.686231255531311), ('total', 0.6836721897125244), ('seventy', 0.6747063398361206), ('upwards', 0.6739497184753418), ('billion', 0.6682729721069336), ('70', 0.666856586933136), ('roughly', 0.6581007838249207)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('percent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### not good choices as the previous set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apparent', 0.6536035537719727), ('serious', 0.618026614189148), ('compelling', 0.6130911111831665), ('notable', 0.6118664145469666), ('clear', 0.605475127696991), ('inevitable', 0.6031302213668823), ('obviously', 0.6028131246566772), ('striking', 0.6021876335144043), ('surprising', 0.5977765321731567), ('pervasive', 0.587814211845398)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('obvious'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some better match such as 'surprising' , 'striking' ,.... so much better job here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### texts from the awards_2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_2002 = []\n",
    "fname_ = []\n",
    "for root,_,files in os.walk(\"abstracts/awards_2002\"):\n",
    "    for fname in files:\n",
    "            if fname[-4:] == \".txt\":\n",
    "                fnames = os.path.join(root, fname)\n",
    "                fname_.append(fnames)\n",
    "for fn in fname_:                \n",
    "    with open(fn,  \"rt\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            for line in f:\n",
    "                if \"Abstract    :\" in line:\n",
    "                    break\n",
    "                # get abstract as a single string\n",
    "        #                             documents = [line.strip() for line in f.readlines()]\n",
    "#                 print(line)\n",
    "                documents = ' '.join([line[:-1].strip() for line in f])\n",
    "                documents = re.sub(' +', ' ', documents)  # remove double spaces\n",
    "                document_2002.append(documents)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic modeling demo\n",
    "# Fast and simple tokenization\n",
    "new_vectorizer_2002 = TfidfVectorizer()\n",
    "word_tokenizer_2002 = new_vectorizer_2002.build_tokenizer()\n",
    "tokenized_text_2002 = [word_tokenizer(doc) for doc in document_2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 16:05:29,711 : INFO : collecting all words and their counts\n",
      "2020-03-17 16:05:29,712 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-17 16:05:30,349 : INFO : collected 110863 word types from a corpus of 3585650 raw words and 9918 sentences\n",
      "2020-03-17 16:05:30,350 : INFO : Loading a fresh vocabulary\n",
      "2020-03-17 16:07:49,787 : INFO : effective_min_count=10 retains 17012 unique words (15% of original 110863, drops 93851)\n",
      "2020-03-17 16:07:49,795 : INFO : effective_min_count=10 leaves 3402739 word corpus (94% of original 3585650, drops 182911)\n",
      "2020-03-17 16:07:49,867 : INFO : deleting the raw counts dictionary of 110863 items\n",
      "2020-03-17 16:07:49,871 : INFO : sample=0.001 downsamples 53 most-common words\n",
      "2020-03-17 16:07:49,871 : INFO : downsampling leaves estimated 2766979 word corpus (81.3% of prior 3402739)\n",
      "2020-03-17 16:07:49,912 : INFO : estimated required memory for 17012 words and 200 dimensions: 35725200 bytes\n",
      "2020-03-17 16:07:49,913 : INFO : resetting layer weights\n",
      "2020-03-17 16:07:52,758 : INFO : training model with 4 workers on 17012 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-17 16:07:53,795 : INFO : EPOCH 1 - PROGRESS: at 36.74% examples, 996580 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 16:07:54,802 : INFO : EPOCH 1 - PROGRESS: at 80.34% examples, 1098113 words/s, in_qsize 7, out_qsize 1\n",
      "2020-03-17 16:07:55,232 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 16:07:55,238 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 16:07:55,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 16:07:55,242 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 16:07:55,243 : INFO : EPOCH - 1 : training on 3585650 raw words (2767178 effective words) took 2.5s, 1120766 effective words/s\n",
      "2020-03-17 16:07:56,262 : INFO : EPOCH 2 - PROGRESS: at 46.67% examples, 1295805 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 16:07:57,273 : INFO : EPOCH 2 - PROGRESS: at 88.05% examples, 1206602 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 16:07:57,547 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 16:07:57,553 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 16:07:57,556 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 16:07:57,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 16:07:57,559 : INFO : EPOCH - 2 : training on 3585650 raw words (2767565 effective words) took 2.3s, 1199049 effective words/s\n",
      "2020-03-17 16:07:58,565 : INFO : EPOCH 3 - PROGRESS: at 57.89% examples, 1619752 words/s, in_qsize 8, out_qsize 1\n",
      "2020-03-17 16:07:59,531 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 16:07:59,532 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 16:07:59,534 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 16:07:59,540 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 16:07:59,540 : INFO : EPOCH - 3 : training on 3585650 raw words (2766312 effective words) took 2.0s, 1399560 effective words/s\n",
      "2020-03-17 16:08:00,549 : INFO : EPOCH 4 - PROGRESS: at 42.99% examples, 1186719 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 16:08:01,552 : INFO : EPOCH 4 - PROGRESS: at 83.30% examples, 1151710 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 16:08:01,954 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 16:08:01,959 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 16:08:01,960 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 16:08:01,963 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 16:08:01,964 : INFO : EPOCH - 4 : training on 3585650 raw words (2766298 effective words) took 2.4s, 1144192 effective words/s\n",
      "2020-03-17 16:08:02,976 : INFO : EPOCH 5 - PROGRESS: at 41.23% examples, 1132152 words/s, in_qsize 8, out_qsize 0\n",
      "2020-03-17 16:08:03,990 : INFO : EPOCH 5 - PROGRESS: at 82.48% examples, 1132885 words/s, in_qsize 7, out_qsize 0\n",
      "2020-03-17 16:08:04,380 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 16:08:04,381 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 16:08:04,384 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 16:08:04,385 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 16:08:04,385 : INFO : EPOCH - 5 : training on 3585650 raw words (2767108 effective words) took 2.4s, 1146098 effective words/s\n",
      "2020-03-17 16:08:04,386 : INFO : training on a 17928250 raw words (13834461 effective words) took 11.6s, 1189882 effective words/s\n"
     ]
    }
   ],
   "source": [
    "## Train word vectors\n",
    "# Make sure you also have cython installed to accelerate computation!\n",
    "import gensim \n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Train word2vec model\n",
    "vectors = gensim.models.Word2Vec(tokenized_text_2002, size=200, window=5, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 16:10:05,185 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('research', 0.8341087102890015), ('study', 0.7794250249862671), ('project', 0.7389976978302002), ('investigation', 0.7211897373199463), ('effort', 0.7016195058822632), ('proposal', 0.6869873404502869), ('investigations', 0.6405994892120361), ('approach', 0.6387003660202026), ('studies', 0.6323292255401611), ('program', 0.6026599407196045)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('work'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('localization', 0.8014732599258423), ('muscle', 0.7841091156005859), ('mRNA', 0.7749961614608765), ('function', 0.7695099115371704), ('variables', 0.7646422982215881), ('channel', 0.7641276121139526), ('curve', 0.764004111289978), ('mechanism', 0.7624053359031677), ('signals', 0.7616889476776123), ('instability', 0.7616028785705566)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('pattern'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('insect', 0.8870453834533691), ('pathogens', 0.88503098487854), ('plants', 0.8498578071594238), ('hormone', 0.8490990400314331), ('reproductive', 0.8456018567085266), ('fungi', 0.8448106050491333), ('sexual', 0.8438753485679626), ('pathogen', 0.8322082757949829), ('resistance', 0.8302956819534302), ('trait', 0.8249731063842773)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('disease'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('70', 0.8721003532409668), ('approximately', 0.8685452938079834), ('80', 0.8582992553710938), ('90', 0.8382930159568787), ('hours', 0.8227778077125549), ('000', 0.8223231434822083), ('50', 0.8207800984382629), ('40', 0.8050470352172852), ('yr', 0.8024305105209351), ('square', 0.7994811534881592)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('percent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ignored', 0.8148432970046997), ('hitherto', 0.8147283792495728), ('serious', 0.810940146446228), ('argued', 0.7716206908226013), ('indeed', 0.7695738077163696), ('certainly', 0.75959312915802), ('surprising', 0.7553567886352539), ('widespread', 0.7388790845870972), ('attracted', 0.737126350402832), ('attempted', 0.7368018627166748)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar('obvious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_whole = {'work': ['research', 'project', 'study','investigation', 'effort','investigations','studies','proposal', 'program', 'approch'],\n",
    "             'pattern':['patterns','shape', 'appearance', 'character', 'temporal', 'patterning','limb', 'organ', 'trajectory', 'phenomenon'], \n",
    "             'disease':['diseases','AIDS','cancer','infectious', 'infections', 'pathogens', 'HIV', 'pathogen','Alzheimer', 'pests'], \n",
    "                         'percent':['approximately', '75', '80',  'kg', 'total', 'seventy', 'upwards', 'billion', '70', 'roughly'], \n",
    "             'obvious':['apparent','serious','compelling', 'notable', 'clear','inevitable', 'obviously', 'striking', 'surprising', 'pervasive'] }\n",
    "\n",
    "\n",
    "\n",
    "dic_2002 = {'work': ['research', 'project', 'study','investigation', 'effort','proposal','studies','investigations', 'program', 'workshop'],\n",
    "             'pattern':['localization','muscle', 'mRNA', 'function', 'variables', 'channel', 'curve', 'mechanism', 'signals', 'instability'], \n",
    "             'disease':['insect', 'pathogens', 'plants', 'hormone', 'reproductive', 'fungi','sexual', 'pathogen', 'resistance', 'trait'], \n",
    "                         'percent':['70','approximately', '80', '90', 'hours', '000', '50',  '40', 'yr', 'square'], \n",
    "             'obvious':['ignored', 'hitherto', 'serious', 'argued', 'indeed', 'certainly', 'surprising', 'widespread', 'attracted','attempted']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work</th>\n",
       "      <th>pattern</th>\n",
       "      <th>disease</th>\n",
       "      <th>percent</th>\n",
       "      <th>obvious</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>research</td>\n",
       "      <td>patterns</td>\n",
       "      <td>diseases</td>\n",
       "      <td>approximately</td>\n",
       "      <td>apparent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>project</td>\n",
       "      <td>shape</td>\n",
       "      <td>AIDS</td>\n",
       "      <td>75</td>\n",
       "      <td>serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>study</td>\n",
       "      <td>appearance</td>\n",
       "      <td>cancer</td>\n",
       "      <td>80</td>\n",
       "      <td>compelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>investigation</td>\n",
       "      <td>character</td>\n",
       "      <td>infectious</td>\n",
       "      <td>kg</td>\n",
       "      <td>notable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>effort</td>\n",
       "      <td>temporal</td>\n",
       "      <td>infections</td>\n",
       "      <td>total</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>investigations</td>\n",
       "      <td>patterning</td>\n",
       "      <td>pathogens</td>\n",
       "      <td>seventy</td>\n",
       "      <td>inevitable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>studies</td>\n",
       "      <td>limb</td>\n",
       "      <td>HIV</td>\n",
       "      <td>upwards</td>\n",
       "      <td>obviously</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>proposal</td>\n",
       "      <td>organ</td>\n",
       "      <td>pathogen</td>\n",
       "      <td>billion</td>\n",
       "      <td>striking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>program</td>\n",
       "      <td>trajectory</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>70</td>\n",
       "      <td>surprising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>approch</td>\n",
       "      <td>phenomenon</td>\n",
       "      <td>pests</td>\n",
       "      <td>roughly</td>\n",
       "      <td>pervasive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             work     pattern     disease        percent     obvious\n",
       "0        research    patterns    diseases  approximately    apparent\n",
       "1         project       shape        AIDS             75     serious\n",
       "2           study  appearance      cancer             80  compelling\n",
       "3   investigation   character  infectious             kg     notable\n",
       "4          effort    temporal  infections          total       clear\n",
       "5  investigations  patterning   pathogens        seventy  inevitable\n",
       "6         studies        limb         HIV        upwards   obviously\n",
       "7        proposal       organ    pathogen        billion    striking\n",
       "8         program  trajectory   Alzheimer             70  surprising\n",
       "9         approch  phenomenon       pests        roughly   pervasive"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.DataFrame.from_dict(dic_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work</th>\n",
       "      <th>pattern</th>\n",
       "      <th>disease</th>\n",
       "      <th>percent</th>\n",
       "      <th>obvious</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>research</td>\n",
       "      <td>localization</td>\n",
       "      <td>insect</td>\n",
       "      <td>70</td>\n",
       "      <td>ignored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>project</td>\n",
       "      <td>muscle</td>\n",
       "      <td>pathogens</td>\n",
       "      <td>approximately</td>\n",
       "      <td>hitherto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>study</td>\n",
       "      <td>mRNA</td>\n",
       "      <td>plants</td>\n",
       "      <td>80</td>\n",
       "      <td>serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>investigation</td>\n",
       "      <td>function</td>\n",
       "      <td>hormone</td>\n",
       "      <td>90</td>\n",
       "      <td>argued</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>effort</td>\n",
       "      <td>variables</td>\n",
       "      <td>reproductive</td>\n",
       "      <td>hours</td>\n",
       "      <td>indeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>proposal</td>\n",
       "      <td>channel</td>\n",
       "      <td>fungi</td>\n",
       "      <td>000</td>\n",
       "      <td>certainly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>studies</td>\n",
       "      <td>curve</td>\n",
       "      <td>sexual</td>\n",
       "      <td>50</td>\n",
       "      <td>surprising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>investigations</td>\n",
       "      <td>mechanism</td>\n",
       "      <td>pathogen</td>\n",
       "      <td>40</td>\n",
       "      <td>widespread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>program</td>\n",
       "      <td>signals</td>\n",
       "      <td>resistance</td>\n",
       "      <td>yr</td>\n",
       "      <td>attracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>workshop</td>\n",
       "      <td>instability</td>\n",
       "      <td>trait</td>\n",
       "      <td>square</td>\n",
       "      <td>attempted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             work       pattern       disease        percent     obvious\n",
       "0        research  localization        insect             70     ignored\n",
       "1         project        muscle     pathogens  approximately    hitherto\n",
       "2           study          mRNA        plants             80     serious\n",
       "3   investigation      function       hormone             90      argued\n",
       "4          effort     variables  reproductive          hours      indeed\n",
       "5        proposal       channel         fungi            000   certainly\n",
       "6         studies         curve        sexual             50  surprising\n",
       "7  investigations     mechanism      pathogen             40  widespread\n",
       "8         program       signals    resistance             yr   attracted\n",
       "9        workshop   instability         trait         square   attempted"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(dic_2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with comparison of two data frme including the selcte words, it can be seen that the words from builded model on the whole data set are more accurate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.2 ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load ELMo model (takes a little while)\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "\n",
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)\n",
    "        #sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features as sentence vector\n",
    "        #return sess.run(tf.reduce_mean(embeddings,1))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['crane', 'date', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Tensor.graph is meaningless when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-492053bdd265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'play'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0melmo_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mword_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a99b39cef2c8>\u001b[0m in \u001b[0;36melmo_vectors\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#         #sess.run(tf.tables_initializer())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         # return average of ELMo features as sentence vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1165\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \"\"\"\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    266\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections_abc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_attrs_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_AttrsFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     self._mappers = [\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     ]\n\u001b[1;32m    410\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     self._mappers = [\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     ]\n\u001b[1;32m    410\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    303\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 305\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    306\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3609\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3610\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3612\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3686\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3687\u001b[0m       \u001b[0;31m# Actually obj is just the object it's referring to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3688\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3689\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3690\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     raise AttributeError(\n\u001b[0;32m-> 1085\u001b[0;31m         \"Tensor.graph is meaningless when eager execution is enabled.\")\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Tensor.graph is meaningless when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "\n",
    "sents = \"\"\" That bird is a crane.\n",
    "They had to use a crane to lift the object.\n",
    "She had to crane her neck to see the movie.\n",
    "Her favorite fruit to eat is a date.\n",
    "Joe took Alexandria out on a date.\n",
    "Not to date myself, but I remember listening to radio shows as a kid.\n",
    "What is your date of birth?\n",
    "You were right.\n",
    "Make a right turn at the light.\n",
    "Access to clean water is a basic human right.\n",
    "\"\"\".split('\\n')\n",
    "def elmo_vecs(target):\n",
    "    elmo_vecs = elmo_vectors(sents)\n",
    "    word_vecs = []\n",
    "    for i, sent in enumerate(sents):\n",
    "        word_vecs.append(elmo_vecs[i][sent.split().index(target)])\n",
    "        print(\"Sentence:\", sent)\n",
    "        print(\"Vector for '%s':\" % target, word_vecs[-1])\n",
    "        print()\n",
    "    print(\"Word vector size:\", word_vecs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_vecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a72668cf7ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvec_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Similarities between '%s' vector in sentences:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_vecs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vec_size = word_vecs[0].shape[0]\n",
    "print(\"Similarities between '%s' vector in sentences:\" % target)\n",
    "for i in range(1, len(sents)):\n",
    "    print(\"Sent 0-%d:\" % i, cosine_similarity(word_vecs[0].reshape((1,vec_size)), \n",
    "                                              word_vecs[i].reshape((1,vec_size)))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
